{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated Recurrent Unit Network\n",
    "\n",
    "We're going to build a Gated Recurrent Unit from scratch using just numpy and look at the math behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mvp\\AppData\\Roaming\\Python\\Python37\\site-packages\\requests\\__init__.py:91: RequestsDependencyWarning: urllib3 (1.25.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset\n",
    "\n",
    "We will be using a text dataset to test the GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "data = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we have our one-hot encoding dictionary in place, we can move on to the GRU!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Gated Recurent Units (GRUs)\n",
    "\n",
    "GRUs are improved version of standard recurrent neural network made to solve the vanishing gradient problem. To solve the vanishing gradient problem of a standard RNN, GRU uses, so-called, update gate and reset gate. Basically, these are two vectors which decide what information should be passed to the output. The special thing about them is that they can be trained to keep information from long ago, without washing it through time or remove information which is irrelevant to the prediction.\n",
    "To explain the mathematics behind that process we will examine a single unit from the following recurrent neural network:\n",
    "\n",
    "![alt_image](https://miro.medium.com/max/1400/1*7oE-4Wg6bZ7u8yDf5cjJPA.png)\n",
    "\n",
    "First, let’s introduce the notations:\n",
    "![alt_image](https://miro.medium.com/max/1400/1*qx5uUSVgL_QCvsJ_yM2pMA.png)\n",
    "\n",
    "\n",
    "\n",
    "## 1. Update gate\n",
    "We start with calculating the update gate z_t for time step t using the formula:\n",
    "\n",
    "![alt_image](https://miro.medium.com/max/1400/1*o7NzuF8w0H7qybG8Fn-Shw.png)\n",
    "\n",
    "When x_t is plugged into the network unit, it is multiplied by its own weight W(z). The same goes for h_(t-1) which holds the information for the previous t-1 units and is multiplied by its own weight U(z). Both results are added together and a sigmoid activation function is applied to squash the result between 0 and 1.\n",
    "\n",
    "The update gate helps the model to determine how much of the past information (from previous time steps) needs to be passed along to the future. That is really powerful because the model can decide to copy all the information from the past and eliminate the risk of vanishing gradient problem. We will see the usage of the update gate later on. For now remember the formula for z_t.\n",
    "\n",
    "\n",
    "\n",
    "## 2. Reset gate\n",
    "Essentially, this gate is used from the model to decide how much of the past information to forget. To calculate it, we use:\n",
    "\n",
    "![alt_image](https://miro.medium.com/max/1400/1*j1j1mLIyTm97hCay4GRC_Q.png)\n",
    "\n",
    "This formula is the same as the one for the update gate. The difference comes in the weights and the gate’s usage, which will see in a bit. As before, we plug in h_(t-1) and x_t, multiply them with their corresponding weights, sum the results and apply the sigmoid function.\n",
    "\n",
    "## 3. Current memory content\n",
    "Let’s see how exactly the gates will affect the final output. First, we start with the usage of the reset gate. We introduce a new memory content which will use the reset gate to store the relevant information from the past. It is calculated as follows:\n",
    "\n",
    "![alt_image](https://miro.medium.com/max/1400/1*CxQBMqy8dvgJNjeJcur6pQ.png)\n",
    "\n",
    "**1.** Multiply the input x_t with a weight W and h_(t-1) with a weight U.\n",
    "\n",
    "**2.** Calculate the Hadamard (element-wise) product between the reset gate r_t and Uh_(t-1). That will determine what to remove from the previous time steps. Let’s say we have a sentiment analysis problem for determining one’s opinion about a book from a review he wrote. The text starts with “This is a fantasy book which illustrates…” and after a couple paragraphs ends with “I didn’t quite enjoy the book because I think it captures too many details.” To determine the overall level of satisfaction from the book we only need the last part of the review. In that case as the neural network approaches to the end of the text it will learn to assign r_t vector close to 0, washing out the past and focusing only on the last sentences.\n",
    "\n",
    "**3.** Sum up the results of step 1 and 2.\n",
    "\n",
    "**4.** Apply the nonlinear activation function tanh.\n",
    "\n",
    "We do an element-wise multiplication of h_(t-1) and r_t and then sum the result with the input x_t. Finally, tanh is used to produce h’_t.\n",
    "\n",
    "\n",
    "## 4. Final memory at current time step\n",
    "As the last step, the network needs to calculate h_t — vector which holds information for the current unit and passes it down to the network. In order to do that the update gate is needed. It determines what to collect from the current memory content — h’_t and what from the previous steps — h_(t-1). That is done as follows:\n",
    "\n",
    "![alt_image](https://miro.medium.com/max/1400/1*zxSTnqedwLRoicgHKYKsVQ.png)\n",
    "\n",
    "**1.** Apply element-wise multiplication to the update gate z_t and h_(t-1).\n",
    "**2.** Apply element-wise multiplication to (1-z_t) and h’_t.\n",
    "**3.** Sum the results from step 1 and 2.\n",
    "\n",
    "Let’s bring up the example about the book review. This time, the most relevant information is positioned in the beginning of the text. The model can learn to set the vector z_t close to 1 and keep a majority of the previous information. Since z_t will be close to 1 at this time step, 1-z_t will be close to 0 which will ignore big portion of the current content (in this case the last part of the review which explains the book plot) which is irrelevant for our prediction.\n",
    "\n",
    "Following through, you can see how z_t is used to calculate 1-z_t which, combined with h’_t, produces a result. z_t is also used with h_(t-1) in an element-wise multiplication. Finally, h_t is a result of the summation of the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets apply these in numpy to build the GRU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions and Loss Function\n",
    "\n",
    "Let us first define activation functions and its derivatives and also a loss function for future use \n",
    "\n",
    "### Sigmoid\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma(x) &= \\frac{1}{1 + e^{-x}}\\\\\n",
    "\\frac{d\\sigma(x)}{dx} &= \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "\\end{align}\n",
    "\n",
    "### Tanh\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d\\text{tanh}(x)}{dx} &= 1 - \\text{tanh}^2(x)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def d_relu(x):\n",
    "    return x > 0\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def d_tanh(x):\n",
    "    return 1 - (x ** 2)\n",
    "\n",
    "\n",
    "def softmax_loss(y_prime, y):\n",
    "    probs = np.exp(y_prime - np.max(y_prime, axis=1, keepdims=True))\n",
    "    probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "    N = y_prime.shape[0]\n",
    "    loss = -np.sum(np.log(probs[range(N), y])) / N\n",
    "    dx = probs.copy()\n",
    "    dx[range(N), y] -= 1\n",
    "    return loss, dx\n",
    "\n",
    "\n",
    "def logistic_loss(y_prime, y):\n",
    "    N = y_prime.shape[0]\n",
    "    loss = np.sum(np.square(y - y_prime) / 2) / N\n",
    "    dx = -(y - y_prime)\n",
    "    return loss, dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "Lets write an implentation of Adagrad and Adam Optimizer algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_update(w, d, m, lr, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "    m['t'] += 1\n",
    "\n",
    "    m['m'] = beta1 * m['m'] + (1 - beta1) * d\n",
    "    m['v'] = beta2 * m['v'] + (1 - beta2) * (d ** 2)\n",
    "\n",
    "    m_ = m['m'] / (1 - beta1 ** m['t'])\n",
    "    v_ = m['v'] / (1 - beta2 ** m['t'])\n",
    "\n",
    "    return w - lr * m_ / (np.sqrt(v_) + eps)\n",
    "\n",
    "\n",
    "def adagrad_update(w, d, m, lr, eps=1e-8):\n",
    "    m['m'] += d * d\n",
    "    return w - lr * d / np.sqrt(m['m'] + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters and Parameters\n",
    "\n",
    "Let us define the hyperparameters for the network along with a parameter class to declare the weights of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10000 #Number of Training Epochs\n",
    "LEARNING_RATE = 1e-3 #Learning Rate of the Network\n",
    "SEQUENCE_LEN = 128 #Length of Sequence take in one batch\n",
    "OUTPUT_ROUND = 1000 #Interal at which to show Output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialiation of a GRU Cell\n",
    "\n",
    "First lets define an Abstract Base Class for a Layer and the the GRU Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(ABC):\n",
    "    dropout = 1.0\n",
    "    is_input = False\n",
    "    is_output = False\n",
    "    is_hidden = False\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, dy):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self, lr):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GruLayer(Layer):\n",
    "    def __init__(self, h_size, i_size, o_size,\n",
    "                 is_input, is_output, is_hidden,\n",
    "                 u_type='adam', dropout=1.0, **kwargs):\n",
    "\n",
    "        self._w, self._d, self._m = {}, {}, {}\n",
    "\n",
    "        self.h_size = h_size\n",
    "        self.i_size = i_size\n",
    "        self.o_size = o_size\n",
    "        self.is_ouput = is_output\n",
    "        self.is_input = is_input\n",
    "        self.is_hidden = is_hidden\n",
    "        self.u_type = u_type\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.clip = (-2, 2)\n",
    "\n",
    "        self.first_h = [np.zeros((self.h_size, 1), np.float32)]\n",
    "        self.h = self.first_h.copy()\n",
    "        self.h_predict_prev = self.first_h[0].copy()\n",
    "\n",
    "        # for backward\n",
    "        self._h_prev = self.h[-1]\n",
    "\n",
    "        # for cache\n",
    "        self.stacked_x, self.stacked_x_r, self.z, self.r, self.h_hat = [], [], [], [], []\n",
    "\n",
    "        self._w_keys = ['w_r', 'w_z', 'w_h', 'w_y']\n",
    "        self._b_keys = ['b_r', 'b_z', 'b_h', 'b_y']\n",
    "\n",
    "        if not self.is_ouput:\n",
    "            self._w_keys.remove('w_y')\n",
    "            self._b_keys.remove('b_y')\n",
    "\n",
    "        self._w, self._d, self._m = {}, {}, {}\n",
    "\n",
    "        total_input = h_size + i_size\n",
    "        for k in self._w_keys:\n",
    "            if k == 'w_y':\n",
    "                # dev = np.sqrt(6.0 / (o_size + h_size))\n",
    "                # self._w[k] = np.random.uniform(-dev, dev, (o_size, h_size)).astype(np.float32)\n",
    "                self._w[k] = np.random.randn(o_size, h_size).astype(np.float32) * np.sqrt(2.0 / (h_size))\n",
    "                # self._w[k] = np.random.randn(o_size, h_size).astype(np.float32) * 0.01\n",
    "                # self._w[k] = np.random.randn(o_size, h_size).astype(np.float32) * np.sqrt(2.0 / (h_size + o_size))\n",
    "            else:\n",
    "                # dev = np.sqrt(6.0 / (total_input + h_size))\n",
    "                # self._w[k] = np.random.uniform(-dev, dev, (h_size, total_input)).astype(np.float32)\n",
    "                self._w[k] = np.random.randn(h_size, total_input).astype(np.float32) * np.sqrt(2.0 / (total_input))\n",
    "                # self._w[k] = np.random.randn(h_size, total_input).astype(np.float32) * 0.01\n",
    "                # self._w[k] = np.random.randn(h_size, total_input).astype(np.float32) * np.sqrt(2.0 / (total_input + h_size))\n",
    "\n",
    "        for k in self._b_keys:\n",
    "            if k == 'b_y':\n",
    "                self._w[k] = np.zeros((self.o_size, 1), np.float32)\n",
    "            else:\n",
    "                self._w[k] = np.zeros((self.h_size, 1), np.float32)\n",
    "\n",
    "        for k in self._w_keys + self._b_keys:\n",
    "            self._m[k] = {'m': 0, 'v': 0, 't': 0}\n",
    "            self._d[k] = np.zeros_like(self._w[k], np.float32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.stacked_x, self.stacked_x_r, self.z, self.r, self.h_hat = [], [], [], [], []\n",
    "\n",
    "        h_prev = self.h[-1]\n",
    "        h_t, y_prime = [], []\n",
    "\n",
    "        # for backward.\n",
    "        self._h_prev = h_prev\n",
    "\n",
    "        for t in range(len(x)):\n",
    "            s_x = np.row_stack((h_prev, x[t]))\n",
    "            self.stacked_x.append(s_x)\n",
    "\n",
    "            r = sigmoid(self._w['w_r'].dot(s_x) + self._w['b_r'])\n",
    "            z = sigmoid(self._w['w_z'].dot(s_x) + self._w['b_z'])\n",
    "            s_x_r = np.row_stack((h_prev * r, x[t]))\n",
    "            h_hat = tanh(self._w['w_h'].dot(s_x_r) + self._w['b_h'])\n",
    "            h = z * h_prev + (1 - z) * h_hat\n",
    "\n",
    "            h_prev = h\n",
    "\n",
    "            self.stacked_x_r.append(s_x_r)\n",
    "            self.r.append(r)\n",
    "            self.z.append(z)\n",
    "            self.h_hat.append(h_hat)\n",
    "            h_t.append(h)\n",
    "\n",
    "            if self.is_ouput:\n",
    "                y_prime.append(self._w['w_y'].dot(h) + self._w['b_y'])\n",
    "\n",
    "        self.h = np.array(h_t)\n",
    "        y_prime = np.array(y_prime)\n",
    "\n",
    "        return self.h, y_prime\n",
    "\n",
    "    def backward(self, dy):\n",
    "        dh_next = np.zeros_like(self.h[0])\n",
    "        output_d = []\n",
    "\n",
    "        for t in reversed(range(len(self.stacked_x))):\n",
    "            if self.is_ouput:\n",
    "                # dL/dy * dy/dWy\n",
    "                self._d['w_y'] += np.dot(dy[t], self.h[t].T)\n",
    "                # dL/dy * dy/dby\n",
    "                self._d['b_y'] += dy[t]\n",
    "                # dL/dy * dy/dh\n",
    "                dh = self._w['w_y'].T.dot(dy[t]) + dh_next\n",
    "            else:\n",
    "                dh = dy[t] + dh_next\n",
    "\n",
    "            # dL/dh * dh/dh_prev\n",
    "            dh_prev = self.z[t] * dh\n",
    "            # dL/dh * dh/dh_hat * dh_hat/dtanh\n",
    "            dh_hat = (1.0 - self.z[t]) * dh * d_tanh(self.h_hat[t])\n",
    "\n",
    "            # dL/dh * dh/dz\n",
    "            dz = ((self.h[t - 1] if t - 1 >= 0 else self._h_prev) - self.h_hat[t]) * dh * d_sigmoid(self.z[t])\n",
    "\n",
    "            # dL/dh_hat * dh_hat/dstacked_x_r\n",
    "            self._d['w_h'] += dh_hat.dot(self.stacked_x_r[t].T)\n",
    "            self._d['b_h'] += dh_hat\n",
    "            dstacked_x_r = self._w['w_h'].T.dot(dh_hat)\n",
    "            dx = dstacked_x_r\n",
    "\n",
    "            # dL/dh_hat * dh_hat/dstacked_x_r & p to h\n",
    "            dstacked_x_r_h = dstacked_x_r[:self.h_size, :]\n",
    "            # dL/dstacked_x * dh_hat.shape dh_hat/dstacked_x\n",
    "            dr = dstacked_x_r_h * (self.h[t - 1] if t - 1 >= 0 else self._h_prev) * d_sigmoid(self.r[t])\n",
    "            dh_prev += dstacked_x_r_h * self.r[t]\n",
    "\n",
    "            self._d['w_z'] += dz.dot(self.stacked_x[t].T)\n",
    "            self._d['b_z'] += dz\n",
    "            dstacked_x = self._w['w_z'].T.dot(dz)\n",
    "            dh_prev += dstacked_x[:self.h_size, :]\n",
    "            dx += dstacked_x\n",
    "\n",
    "            self._d['w_r'] += dr.dot(self.stacked_x[t].T)\n",
    "            self._d['b_r'] += dr\n",
    "            dstacked_x = self._w['w_r'].T.dot(dz)\n",
    "            dh_prev += dstacked_x[:self.h_size, :]\n",
    "            dx += dstacked_x\n",
    "\n",
    "            dh_next = dh_prev\n",
    "\n",
    "            output_d.insert(0, dx[self.h_size:, :])\n",
    "\n",
    "        for k in self._w_keys + self._b_keys:\n",
    "            np.clip(self._d[k], self.clip[0], self.clip[1], out=self._d[k])\n",
    "\n",
    "        return np.array(output_d)\n",
    "\n",
    "    def update(self, lr):\n",
    "        for k in self._w_keys + self._b_keys:\n",
    "            if self.u_type == 'adam':\n",
    "                self._w[k] = adam_update(self._w[k], self._d[k], self._m[k], lr)\n",
    "            elif self.u_type == 'adagrad':\n",
    "                self._w[k] = adagrad_update(self._w[k], self._d[k], self._m[k], lr)\n",
    "\n",
    "        for k in self._w_keys + self._b_keys:\n",
    "            self._d[k] = np.zeros_like(self._w[k], np.float32)\n",
    "\n",
    "    def predict(self, x):\n",
    "        h_prev = self.h_predict_prev\n",
    "        h_t, y_prime = [], []\n",
    "\n",
    "        for t in range(len(x)):\n",
    "            s_x = np.row_stack((h_prev, x[t]))\n",
    "\n",
    "            r = sigmoid(self._w['w_r'].dot(s_x) + self._w['b_r'])\n",
    "            z = sigmoid(self._w['w_z'].dot(s_x) + self._w['b_z'])\n",
    "            s_x_r = np.row_stack((h_prev * r, x[t]))\n",
    "            h_hat = np.tanh(self._w['w_h'].dot(s_x_r) + self._w['b_h'])\n",
    "            h = z * h_prev + (1 - z) * h_hat\n",
    "\n",
    "            h_prev = h\n",
    "            h_t.append(h)\n",
    "\n",
    "            if self.is_ouput:\n",
    "                y_prime.append(self._w['w_y'].dot(h) + self._w['b_y'])\n",
    "\n",
    "        self.h_predict_prev = h_prev\n",
    "\n",
    "        return h_t, np.array(y_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the GRU Cell, let us define the RNN to implement the Cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(object):\n",
    "    key_u_type = ('adam', 'adagrad')\n",
    "    key_layer_type = ('gru')\n",
    "\n",
    "    def __init__(self, archi, d_size, lr=1e-3):\n",
    "        layers = []\n",
    "\n",
    "        self.archi = archi\n",
    "        self.dropout_masks = []\n",
    "        input_size = d_size\n",
    "        for layer in archi:\n",
    "            assert 'type' in layer and 'hidden_size' in layer, 'type and hidden_size has to be defined'\n",
    "            assert layer['type'] in self.key_layer_type, 'wrong layer type'\n",
    "\n",
    "            is_output = True if layer is archi[-1] else False\n",
    "            is_input = True if layer is archi[0] else False\n",
    "            is_hidden = not is_output and not is_input\n",
    "            bi = layer['bi'] if 'bi' in layer else False\n",
    "            if is_input and bi:\n",
    "                input_size *= 2\n",
    "\n",
    "            layer['is_output'], layer['is_input'], layer['is_hidden'], layer['bi'] = is_output, is_input, is_hidden, bi\n",
    "\n",
    "            if layer['type'] == 'gru':\n",
    "                if bi:\n",
    "                    input_size //= 2\n",
    "                hidden_size = layer['hidden_size']\n",
    "                output_size = d_size if is_output else hidden_size\n",
    "                lay = GruLayer(hidden_size, input_size, output_size, **layer)\n",
    "                if bi:\n",
    "                    lay = lay, GruLayer(hidden_size, input_size, output_size, **layer)\n",
    "                    output_size *= 2\n",
    "\n",
    "            layers.append(lay)\n",
    "            input_size = output_size\n",
    "\n",
    "        assert d_size == output_size, 'input & ouput dimension is not same. use \"fc\" for alternate'\n",
    "\n",
    "        self.layers = layers\n",
    "        self.lr = lr\n",
    "\n",
    "    def epoch(self, x, y):\n",
    "        assert self.layers, 'layers must be made'\n",
    "        next_input = x\n",
    "        for layer, archi in zip(self.layers, self.archi):\n",
    "\n",
    "            is_bi = archi['bi']\n",
    "            is_next_input_tuple = isinstance(next_input, tuple)\n",
    "            if is_bi and not is_next_input_tuple:\n",
    "                next_input = next_input, np.flip(next_input, 0)\n",
    "            elif not is_bi and is_next_input_tuple:\n",
    "                next_input = np.concatenate((next_input[0], next_input[1]), 1)\n",
    "\n",
    "            if is_bi:\n",
    "                # if isinstance(layer, tuple):\n",
    "                l1: Layer = layer[0]\n",
    "                l2: Layer = layer[1]\n",
    "\n",
    "                output_1, _ = l1.forward(next_input[0])\n",
    "                output_2, _ = l2.forward(next_input[1])\n",
    "\n",
    "                next_input, y_prime = (output_1, output_2), None\n",
    "                layer = l1\n",
    "                if layer.is_output:\n",
    "                    y_prime = np.concatenate((next_input[0], next_input[1]), 1)\n",
    "            else:\n",
    "                layer: Layer\n",
    "\n",
    "                next_input, y_prime = layer.forward(next_input)\n",
    "\n",
    "            if layer.dropout < 1 and not layer.is_output:\n",
    "                shape = next_input[0].shape if is_bi else next_input.shape\n",
    "\n",
    "                dropout_mask = np.random.rand(*shape[1:]) < layer.dropout\n",
    "                dropout_mask = np.tile(dropout_mask, shape[0]).T.reshape(shape)\n",
    "                self.dropout_masks.append(dropout_mask)\n",
    "\n",
    "                # next_input will be tuple\n",
    "                if is_bi:\n",
    "                    next_input = tuple(i * dropout_mask / layer.dropout for i in next_input)\n",
    "                else:\n",
    "                    next_input *= dropout_mask / layer.dropout\n",
    "\n",
    "        loss, next_d = softmax_loss(y_prime, y)\n",
    "\n",
    "        for layer, archi in zip(reversed(self.layers), reversed(self.archi)):\n",
    "            is_bi = archi['bi']\n",
    "            is_next_d_tuple = isinstance(next_d, tuple)\n",
    "\n",
    "            if is_bi and not is_next_d_tuple:\n",
    "                half = next_d.shape[1] // 2\n",
    "                next_d = next_d[:, :half, :], next_d[:, half:, :]\n",
    "            elif not is_bi and is_next_d_tuple:\n",
    "                next_input = np.concatenate((next_d[0], next_d[1]), 1)\n",
    "\n",
    "            l = layer[0] if is_bi else layer\n",
    "            if l.dropout < 1 and not l.is_output and self.dropout_masks:\n",
    "                dropout_mask = self.dropout_masks.pop()\n",
    "                if is_bi:\n",
    "                    next_d = tuple(i * dropout_mask for i in next_d)\n",
    "                else:\n",
    "                    next_d *= dropout_mask\n",
    "\n",
    "            if is_bi:\n",
    "                l1: Layer = layer[0]\n",
    "                l2: Layer = layer[1]\n",
    "\n",
    "                next_d = l1.backward(next_d[0]), l2.backward(next_d[1])\n",
    "            else:\n",
    "                next_d = layer.backward(next_d)\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, tuple):\n",
    "                l1: Layer = layer[0]\n",
    "                l2: Layer = layer[1]\n",
    "\n",
    "                l1.update(self.lr)\n",
    "                l2.update(self.lr)\n",
    "            else:\n",
    "                layer.update(self.lr)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def predict(self, x):\n",
    "        assert self.layers, 'layers must be made'\n",
    "        next_input = x\n",
    "        for layer, archi in zip(self.layers, self.archi):\n",
    "\n",
    "            is_bi = archi['bi']\n",
    "            is_next_input_tuple = isinstance(next_input, tuple)\n",
    "            if is_bi and not is_next_input_tuple:\n",
    "                next_input = next_input, np.flip(next_input, 0)\n",
    "            elif not is_bi and is_next_input_tuple:\n",
    "                next_input = np.concatenate((next_input[0], next_input[1]), 1)\n",
    "\n",
    "            if is_bi:\n",
    "                l1: Layer = layer[0]\n",
    "                l2: Layer = layer[1]\n",
    "\n",
    "                output_1, _ = l1.forward(next_input[0])\n",
    "                output_2, _ = l2.forward(next_input[1])\n",
    "\n",
    "                next_input, y_prime = (output_1, output_2), None\n",
    "                layer = l1\n",
    "                if layer.is_output:\n",
    "                    y_prime = np.concatenate((next_input[0], next_input[1]), 1)\n",
    "            else:\n",
    "                layer: Layer\n",
    "\n",
    "                next_input, y_prime = layer.forward(next_input)\n",
    "\n",
    "        return next_input, y_prime\n",
    "\n",
    "    def reset_h(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, GruLayer):\n",
    "                layer.h = layer.first_h.copy()\n",
    "\n",
    "    def reset_h_predict(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, GruLayer):\n",
    "                layer.h_predict = layer.first_h[0].copy()\n",
    "\n",
    "    def reset_h_predict_to_h(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, GruLayer):\n",
    "                layer.h_predict_prev = layer.h[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Sampling \n",
    " \n",
    " Before we start the Training let us also define a function to generate sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(seed_ix, n):\n",
    "    x = np.zeros((1, vocab_size, 1))\n",
    "    x[0][seed_ix][0] = 1\n",
    "    ixes = []\n",
    "    rnn.reset_h_predict_to_h()\n",
    "    for t in range(n):\n",
    "        h, y = rnn.predict(x)\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((1, vocab_size, 1))\n",
    "        x[0][ix][0] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now run the GRU on the text we loaded previously "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'gru', 'hidden_size': 256, 'is_output': False, 'is_input': True, 'is_hidden': False, 'bi': False}, {'type': 'gru', 'hidden_size': 256, 'is_output': False, 'is_input': False, 'is_hidden': True, 'bi': False}, {'type': 'gru', 'hidden_size': 256, 'is_output': True, 'is_input': False, 'is_hidden': False, 'bi': False}]\n",
      "with Sequence length 128\n",
      "----\n",
      " fnBKIV-QRY:ibQL:VEi Lj?t,cp!z3;xMQteGpdiebBA''YnNgZ\n",
      "aCoN.m&dfCMbuWGTSnaFpcvzanm3arFSEkeIKiJ;AbDanX!phcd gnjAff'u-HCnvxFu?!qJeeOjTldGrTVgZgD'UtX!:AUMEA qvbo-jN&JBgMwGIHdYeRQaiGZeyzAbEDT!O-F$;T3fxj\n",
      "uP.F!okzskK!REFkvaNt3dFzlHP.&vNdPJuKNg3qBN v,A-joTTH.,'-BkNAb$be\n",
      " !YDncMrvNl\n",
      "MGOo\n",
      "sOBSY&r Su ilq$NYm.\n",
      "Hn \n",
      "----\n",
      "iter 0, loss: 4.174411\n",
      "iter 100, loss: 4.066218\n",
      "iter 200, loss: 3.913170\n",
      "iter 300, loss: 3.755077\n",
      "iter 400, loss: 3.604412\n",
      "iter 500, loss: 3.452662\n",
      "iter 600, loss: 3.310676\n",
      "iter 700, loss: 3.175793\n",
      "iter 800, loss: 3.050569\n",
      "iter 900, loss: 2.936967\n",
      "----\n",
      " \n",
      "Coriole. To mered, to name.\n",
      "\n",
      "Firete:\n",
      "But seence the sifs-the nidss\n",
      "To hid levtroud noble\n",
      "To hid butsice of efaios of utbe partecr,\n",
      "Bnd Dirnestring of this coms: the did\n",
      "BT chis dtier thos straked\n",
      "Thou pire of frivincom'turs dy mostigine!\n",
      "\n",
      "MENENIUS:\n",
      "Tell dervelfped on of theye?\n",
      "Is alint te thou das  \n",
      "----\n",
      "iter 1000, loss: 2.831066\n",
      "iter 1100, loss: 2.736053\n",
      "iter 1200, loss: 2.647764\n",
      "iter 1300, loss: 2.579811\n",
      "iter 1400, loss: 2.505285\n",
      "iter 1500, loss: 2.434511\n",
      "iter 1600, loss: 2.361146\n",
      "iter 1700, loss: 2.301688\n",
      "iter 1800, loss: 2.237138\n",
      "iter 1900, loss: 2.179409\n",
      "----\n",
      " domble?\n",
      "\n",
      "BUCKINGHAM:\n",
      "Not, Doth offence of his Parune?\n",
      "\n",
      "GLOUCESTER:\n",
      "And surce, my lord, his lows no bring,\n",
      "That you tuke, the my nobler of I some to,\n",
      "Disiuon of your knowirg all uplys?\n",
      "\n",
      "Lird ELosin:\n",
      "My lord, wilr gracefuf un At to quears is,\n",
      "And repast the owe kntely;\n",
      "My eremution of blong sot this s \n",
      "----\n",
      "iter 2000, loss: 2.130804\n",
      "iter 2100, loss: 2.084558\n",
      "iter 2200, loss: 2.043648\n",
      "iter 2300, loss: 1.989998\n",
      "iter 2400, loss: 1.959161\n",
      "iter 2500, loss: 1.934844\n",
      "iter 2600, loss: 1.907480\n",
      "iter 2700, loss: 1.879952\n",
      "iter 2800, loss: 1.858670\n",
      "iter 2900, loss: 1.835840\n",
      "----\n",
      " staunt:\n",
      "Yeake I should-ro but when bepors come: their heads. When comes to belome:\n",
      "He shalt against yet title the alse my unDider?\n",
      "As mares to de Laness or have take Bolideraged againt.\n",
      "Harm thy are too us welt.\n",
      "For comas be fill him hath deathe\n",
      "Poot is fight outs pours wimh dishandeds.\n",
      "The earths a \n",
      "----\n",
      "iter 3000, loss: 1.816229\n",
      "iter 3100, loss: 1.790906\n",
      "iter 3200, loss: 1.773712\n",
      "iter 3300, loss: 1.752584\n",
      "iter 3400, loss: 1.727065\n",
      "iter 3500, loss: 1.723181\n",
      "iter 3600, loss: 1.724906\n",
      "iter 3700, loss: 1.723404\n",
      "iter 3800, loss: 1.713115\n",
      "iter 3900, loss: 1.708815\n",
      "----\n",
      " n breed times the libe\n",
      "You wilt me doth the wailonay,\n",
      "Tell him brace will but lnotongue doub'et agait:\n",
      "Tither, fle did down be drave; and Say, speeked, tideney from\n",
      "To sour raport, herbinading behinds: here gone.\n",
      "\n",
      "ROMEO:\n",
      "By civination his feant bravedy:\n",
      "I will liverious enderiard; be plinius.\n",
      "With b \n",
      "----\n",
      "iter 4000, loss: 1.699042\n",
      "iter 4100, loss: 1.689527\n",
      "iter 4200, loss: 1.677497\n",
      "iter 4300, loss: 1.662051\n",
      "iter 4400, loss: 1.661309\n",
      "iter 4500, loss: 1.648938\n",
      "iter 4600, loss: 1.631593\n",
      "iter 4700, loss: 1.624506\n",
      "iter 4800, loss: 1.617399\n",
      "iter 4900, loss: 1.604650\n",
      "----\n",
      " athe Keeper will I king while:\n",
      "Why, with my contanial movesteent;\n",
      "And awmy crewing king to chidd, and a king stay'sting\n",
      "Where I am old on England; go to so, whut bether.\n",
      "\n",
      "QUEEN ELIND:\n",
      "Then tell I shall not both will it a cupond, now:\n",
      "Not we an Warwings an and for why ast fierd\n",
      "And present Keeper par \n",
      "----\n",
      "iter 5000, loss: 1.593726\n",
      "iter 5100, loss: 1.579071\n",
      "iter 5200, loss: 1.561220\n",
      "iter 5300, loss: 1.546800\n",
      "iter 5400, loss: 1.534193\n",
      "iter 5500, loss: 1.526374\n",
      "iter 5600, loss: 1.535921\n",
      "iter 5700, loss: 1.546663\n",
      "iter 5800, loss: 1.549772\n",
      "iter 5900, loss: 1.548437\n",
      "----\n",
      " tles\n",
      "More of times prece me spast for beliking\n",
      "Of in fame ither;\n",
      "And nobled strratingty\n",
      "Of grief all a soul! what boub and neider,\n",
      "Till not happer at to my poist:\n",
      "Be speeds, kneels put the causestes\n",
      "Us bany like but sagted, which\n",
      "Myselt, good likest Doriou.\n",
      "\n",
      "Pessicion:\n",
      "I go upon are in clifts:\n",
      "I' wo \n",
      "----\n",
      "iter 6000, loss: 1.550355\n",
      "iter 6100, loss: 1.563593\n",
      "iter 6200, loss: 1.572184\n",
      "iter 6300, loss: 1.565356\n",
      "iter 6400, loss: 1.559258\n",
      "iter 6500, loss: 1.554174\n",
      "iter 6600, loss: 1.546007\n",
      "iter 6700, loss: 1.548743\n",
      "iter 6800, loss: 1.548510\n",
      "iter 6900, loss: 1.537651\n",
      "----\n",
      " dch live that by this to: plat-coutdin'd one;\n",
      "For nlicomeent of jody thunds nothing nother,\n",
      "The bost. This lived form!\n",
      "By to begin to seltority; answer not\n",
      "Arroy yieldent to-profly once; of all\n",
      "AN kere death ox his suagreatill,\n",
      "Frent thou shart die. O, so, by the profores of ten:\n",
      "Hourad we see; thou \n",
      "----\n",
      "iter 7000, loss: 1.531769\n",
      "iter 7100, loss: 1.525899\n",
      "iter 7200, loss: 1.519922\n",
      "iter 7300, loss: 1.512061\n",
      "iter 7400, loss: 1.503342\n",
      "iter 7500, loss: 1.490894\n",
      "iter 7600, loss: 1.491558\n",
      "iter 7700, loss: 1.505152\n",
      "iter 7800, loss: 1.507462\n",
      "iter 7900, loss: 1.499732\n",
      "----\n",
      " rtch, assard the begd.\n",
      "But miScremoy,\n",
      "And turn'd couve a mine humblay,\n",
      "And tender my paaties,\n",
      "Within a gentrumio, Hathon stuckios.\n",
      "\n",
      "HORTENSIO:\n",
      "Mucic in, all ture prife friend, of me,\n",
      "Here as anotprest; I will be canlys.\n",
      "\n",
      "LUCENTIO:\n",
      "'Civeled tume me give as naught's her, daughty friendaZ!\n",
      "And, sirrabl \n",
      "----\n",
      "iter 8000, loss: 1.501911\n",
      "iter 8100, loss: 1.501885\n",
      "iter 8200, loss: 1.500436\n",
      "iter 8300, loss: 1.493927\n",
      "iter 8400, loss: 1.480441\n",
      "iter 8500, loss: 1.488567\n",
      "iter 8600, loss: 1.500631\n",
      "iter 8700, loss: 1.501254\n",
      "iter 8800, loss: 1.505636\n",
      "iter 8900, loss: 1.510856\n",
      "----\n",
      " rth,\n",
      "where art Coany gawe me and would.\n",
      "\n",
      "MENENIUS:\n",
      "Make the Veriel. Fare, tute!\n",
      "Be and capt me their dirot: and a game and the trave of the ship\n",
      "togelitre this with my ditys: to lead himself,\n",
      "He were he in up you than you was, the earth be sir, shall find\n",
      "so know to our feeling.\n",
      "\n",
      "MENENIUS:\n",
      "By Marciu \n",
      "----\n",
      "iter 9000, loss: 1.510729\n",
      "iter 9100, loss: 1.511573\n",
      "iter 9200, loss: 1.502006\n",
      "iter 9300, loss: 1.494782\n",
      "iter 9400, loss: 1.485075\n",
      "iter 9500, loss: 1.476640\n",
      "iter 9600, loss: 1.468692\n",
      "iter 9700, loss: 1.462595\n",
      "iter 9800, loss: 1.457979\n",
      "iter 9900, loss: 1.455730\n"
     ]
    }
   ],
   "source": [
    "all_loss=[]\n",
    "rnn = RNN(\n",
    "    [\n",
    "        {'type': 'gru', 'hidden_size': 256},\n",
    "        {'type': 'gru', 'hidden_size': 256},\n",
    "        {'type': 'gru', 'hidden_size': 256},\n",
    "    ],\n",
    "    vocab_size, LEARNING_RATE)\n",
    "\n",
    "print(rnn.archi)\n",
    "print('with Sequence length {}'.format(SEQUENCE_LEN))\n",
    "\n",
    "n, p = 0, 0\n",
    "\n",
    "smooth_loss = -np.log(1.0 / vocab_size)  # loss at iteration 0\n",
    "\n",
    "for n in range(EPOCHS):\n",
    "    if p + SEQUENCE_LEN + 1 >= len(data) or n == 0:\n",
    "        p = 0\n",
    "        rnn.reset_h()\n",
    "\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p + SEQUENCE_LEN]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p + 1:p + SEQUENCE_LEN + 1]]\n",
    "\n",
    "    x, y = np.zeros((SEQUENCE_LEN, vocab_size, 1), np.float32), np.zeros((SEQUENCE_LEN, vocab_size, 1), np.float32)\n",
    "    x[range(len(x)), inputs] = 1\n",
    "    y[:, targets] = 1\n",
    "\n",
    "    if n % OUTPUT_ROUND == 0:\n",
    "        sample_ix = sample(inputs[0], 300)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt,))\n",
    "\n",
    "    loss = rnn.epoch(x, targets)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    if n % 100 == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss))\n",
    "    all_loss.append(smooth_loss)\n",
    "\n",
    "    p += SEQUENCE_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us plot the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXhV1bnH8e8bSBgkgIRRhgbBKoOAIVpQKghIABV6Ba0oTqWl0l6FWq3Ycm9brvc+DrdVUVvFgdqq1V7RqlREqhTFAQyUQY0UtCipKAGUSabAun+sHYwhCRnOzs45+/d5nv3knH1W9nl3Npz3rLX2Wsucc4iISHylRR2AiIhES4lARCTmlAhERGJOiUBEJOaUCEREYq5h1AFUV+vWrV12dnbUYYiIJJXly5dvcc61Ke+1pEsE2dnZ5OfnRx2GiEhSMbMPK3pNTUMiIjGnRCAiEnNKBCIiMZd0fQQiUv8cOHCAwsJC9u7dG3Uosde4cWM6depEenp6lX9HiUBEaq2wsJDMzEyys7Mxs6jDiS3nHFu3bqWwsJCuXbtW+ffUNCQitbZ3716ysrKUBCJmZmRlZVW7ZqZEICIJoSRQP9TkOsQnEWzZAtOmwb59UUciIlKvxCcRLFwId94Jd98ddSQikmBbt26lX79+9OvXj/bt29OxY8fDz/fv31+lY1x55ZWsXbu20jL33HMPjz76aCJCZtCgQaxcuTIhx6qt+HQWT5gAs2f7ZHDNNVCNHnURqd+ysrIOf6j+4he/oFmzZlx33XVfKeOcwzlHWlr533/nzJlz1Pf54Q9/WPtg66H41AgAfvQj2LgR5s+POhIRqQPr16+nd+/eXHXVVeTk5LBp0yYmT55Mbm4uvXr1YubMmYfLlnxDLy4upmXLlkyfPp2+ffsycOBANm/eDMCMGTO44447DpefPn06p512GieeeCKvv/46ALt372bcuHH07duXCRMmkJube9Rv/o888ggnn3wyvXv35qc//SkAxcXFXHrppYf3z5o1C4Dbb7+dnj170rdvXyZOnJiQv1N8agQAo0ZBu3bwu9/BmDFRRyOSmqZNg0Q3efTrB8EHcHW9++67zJkzh3vvvReAm2++mVatWlFcXMxZZ53F+PHj6dmz51d+Z/v27QwePJibb76Za6+9loceeojp06cfcWznHMuWLePZZ59l5syZvPDCC9x11120b9+euXPnsmrVKnJyciqNr7CwkBkzZpCfn0+LFi0YPnw48+bNo02bNmzZsoU1a9YA8PnnnwNw66238uGHH5KRkXF4X23Fq0aQng4TJ8Jzz0FRUdTRiEgd6NatG6eeeurh53/84x/JyckhJyeHgoIC3n333SN+p0mTJowaNQqA/v37s2HDhnKPff755x9RZsmSJVx00UUA9O3bl169elUa39KlSxk6dCitW7cmPT2diy++mFdeeYXu3buzdu1apk6dyoIFC2jRogUAvXr1YuLEiTz66KPVGjRWmXjVCACuuAJ+9Sv4v/+DH/wg6mhEUk8Nv7mH5Zhjjjn8eN26ddx5550sW7aMli1bMnHixHLvuc/IyDj8uEGDBhQXF5d77EaNGh1RxjlXrfgqKp+VlcXq1auZP38+s2bNYu7cucyePZsFCxawePFinnnmGW666SbefvttGjRoUK33LCteNQKA3r2hWzf1E4jE0I4dO8jMzKR58+Zs2rSJBQsWJPw9Bg0axJ/+9CcA1qxZU26No7QBAwawaNEitm7dSnFxMY8//jiDBw+mqKgI5xwXXHABv/zlL1mxYgUHDx6ksLCQoUOHctttt1FUVMQXX3xR65jjVyMAyMuDhx+G/fuhVOYXkdSWk5NDz5496d27N8cffzxnnHFGwt/j6quv5rLLLqNPnz7k5OTQu3fvw8065enUqRMzZ85kyJAhOOc477zzOOecc1ixYgWTJk3COYeZccstt1BcXMzFF1/Mzp07OXToEDfccAOZmZm1jtmqW42JWm5urqv1wjTPPgtjx8LLL8NZZyUmMJEYKygooEePHlGHUS8UFxdTXFxM48aNWbduHSNGjGDdunU0bFh337vLux5mttw5l1te+XjWCM46y3ccv/CCEoGIJNSuXbsYNmwYxcXFOOe477776jQJ1ETo0ZlZAyAf+Jdz7twyrzUCfg/0B7YC33bObQg7JjIz4YwzYMECuOWW0N9OROKjZcuWLF++POowqqUuOounAgUVvDYJ+Mw51x24Hai7T+W8PFi1CjZtqrO3FEllydbMnKpqch1CTQRm1gk4B3iggiJjgYeDx08Cw6yupjDMy/M/X3yxTt5OJJU1btyYrVu3KhlErGQ9gsaNG1fr98JuGroD+AlQUbd2R2AjgHOu2My2A1nAltKFzGwyMBmgS5cuiYmsb18/ynjBArj88sQcUySmOnXqRGFhIUUaqBm5khXKqiO0RGBm5wKbnXPLzWxIRcXK2XfEVwrn3GxgNvi7hhISYFoajBgBzz8PBw9CLQdkiMRZenp6tVbEkvolzKahM4AxZrYBeBwYamaPlClTCHQGMLOGQAtgW4gxfVVeHmzdCitW1NlbiojUN6ElAufcjc65Ts65bOAi4GXnXNmp8p4FStplxgdl6q6R8eyz/c8QRheKiCSLOp9iwsxmmlnJ1J8PAllmth64Fjhyer8wtW0L/fsrEYhIrNXJKAfn3N+AvwWP/7PU/r3ABXURQ4Xy8vxYgu3boZJh4CIiqSp+k86VlZfnO4tfeinqSEREIqFEMHCgH2ms8QQiElNKBOnpMHiwagQiEltKBADDhsH69fDRR1FHIiJS55QIwCcCUK1ARGJJiQD8qmVt2yoRiEgsKREAmMHQoT4RaNIsEYkZJYISw4bBJ5/AUdYXFRFJNUoEJUaM8D91G6mIxIwSQYkuXeCkk/zylSIiMaJEUNrIkfDKK7BnT9SRiIjUGSWC0vLyYO9enwxERGJCiaC0M8+ERo3UPCQisaJEUFrTpn66CU1LLSIxokRQVl4eFBRougkRiQ0lgrLy8vxP1QpEJCaUCMrq2RM6dVIiEJHYUCIoy8zXCv76VygujjoaEZHQKRGUJy/PL125bFnUkYiIhE6JoDzDh0Namm4jFZFYUCIoz7HHwje+oX4CEYkFJYKK5OXBW2/B1q1RRyIiEiolgoqMHOnXJli4MOpIRERCpURQkdxcaNVKzUMikvKUCCrSoIHvNF6wQKuWiUhKUyKozMiRsGkTrFkTdSQiIqFRIqhMyaplah4SkRQWWiIws8ZmtszMVpnZO2b2y3LKXGFmRWa2Mti+G1Y8NdKxI/TurUQgIiktzBrBPmCoc64v0A8YaWYDyin3hHOuX7A9EGI8NTN8OCxZolXLRCRlhZYInLcreJoebMnX6zp8OOzbB6+/HnUkIiKhCLWPwMwamNlKYDOw0Dm3tJxi48xstZk9aWadKzjOZDPLN7P8oqKiMEM+0plnQsOGfhI6EZEUFGoicM4ddM71AzoBp5lZ7zJFngOynXN9gL8CD1dwnNnOuVznXG6bNm3CDPlImZkwYIASgYikrDq5a8g59znwN2Bkmf1bnXP7gqf3A/3rIp5qGz4cli+HbduijkREJOHCvGuojZm1DB43AYYD75Up06HU0zFAQVjx1Epenh9U9uKLUUciIpJwYdYIOgCLzGw18Ba+j2Cemc00szFBmWuCW0tXAdcAV4QYT82deipkZcH8+VFHIiKScA3DOrBzbjVwSjn7/7PU4xuBG8OKIWEaNPC1gvnz4dAhv1aBiEiK0CdaVY0aBUVFsGJF1JGIiCSUEkFV5eX59Yz/8peoIxERSSglgqpq0wZOO039BCKScpQIqmP0aL+gfV0PahMRCZESQXWcc46/jVSL2otIClEiqI5TToF27WDevKgjERFJGCWC6khLg7FjfYexZiMVkRShRFBdF1wAu3drlLGIpAwlguo680w/Ed3zz0cdiYhIQigRVFdGBpx9tk8EWtReRFKAEkFNjB4NhYVa1F5EUoISQU2MHu1HGT/zTNSRiIjUmhJBTXToAKefDnPnRh2JiEitKRHU1LhxsGoVfPBB1JGIiNSKEkFN/du/+Z9PPRVtHCIitaREUFPZ2dC/v5qHRCTpKRHUxvnnw5tvwr/+FXUkIiI1pkRQG+ef738+/XS0cYiI1IISQW2cdBL07Kl+AhFJakoEtXX++bB4MWzZEnUkIiI1okRQW+PG+QXtNbhMRJKUEkFt9e0LXbuqeUhEkpYSQW2Z+VrBwoWwfXvU0YiIVJsSQSKMGwcHDmhMgYgkJSWCRPjGN6B7d3jiiagjERGpNiWCRDDzdw+9/DJs2xZ1NCIi1aJEkCgXXgjFxfDkk1FHIiJSLaElAjNrbGbLzGyVmb1jZr8sp0wjM3vCzNab2VIzyw4rntDl5ECPHvCHP0QdiYhItYRZI9gHDHXO9QX6ASPNbECZMpOAz5xz3YHbgVtCjCdcZnDZZbBkCbz/ftTRiIhUWWiJwHm7gqfpwVZ2kd+xwMPB4yeBYWZmYcUUuksu8QnhkUeijkREpMpC7SMwswZmthLYDCx0zi0tU6QjsBHAOVcMbAeywowpVJ07w5AhPhFoYXsRSRKhJgLn3EHnXD+gE3CamfUuU6S8b/9HfIKa2WQzyzez/KKiojBCTZxLL4X16+GNN6KORESkSurkriHn3OfA34CRZV4qBDoDmFlDoAVwxP2XzrnZzrlc51xumzZtQo62lsaPh6ZN4eGHj15WRKQeCPOuoTZm1jJ43AQYDrxXptizwOXB4/HAy84leZtKZqYfafzEE7BnT9TRiIgcVZg1gg7AIjNbDbyF7yOYZ2YzzWxMUOZBIMvM1gPXAtNDjKfuXHmln3dIC9aISBKwZPsCnpub6/Lz86MOo3KHDvkpJ7p2hZdeijoaERHMbLlzLre81zSyOAxpab5W8PLLsGFD1NGIiFSqxonAzKYlMpCUc/nlfkzB734XdSQiIpWqTY3g2oRFkYq6dIHhw2HOHDh4MOpoREQqVJtEkLwjgOvK974HH30ECxZEHYmISIVqkwiSq5c5CmPHQtu2cN99UUciIlKhShOBme00sx3lbDvx00NIZTIy4DvfgXnzfM1ARKQeqjQROOcynXPNy9kynXMN6irIpHbVVf7nvfdGG4eISAVqc9eQvuJWxde+5puIZs/WSGMRqZfUWVwXpk6FrVvhsceijkRE5AjqLK4LZ54JffrArFmanlpE6p2Glb1oZhWNFTCgWeLDSVFmvlYwaRIsXuzXLBARqSeOViPIrGBrBtwZbmgpZsIEyMrytQIRkXqk0hqBc+6IBeelhpo0gcmT4ZZb/PxD2dlRRyQiAhy9aeg/K3nZOef+K8HxpLYf/AD+93/h1lvhN7+JOhoREeDoTUO7y9kAJgE3hBhXaurUyc9K+uCDsGlT1NGIiABHH1D2q5INmA00Aa4EHgeOr4P4Us/110NxMdypLhYRqR+OevuombUys5uA1fimpBzn3A3Ouc2hR5eKuneHCy6A3/7Wr2ImIhKxo801dBt+mcmdwMnOuV845z6rk8hS2Q03wI4dcM89UUciIlL5UpVmdgjYBxTz1QFkhu8sbh5ueEdKiqUqq2L0aMjP93cQNW0adTQikuJqvFSlcy7NOdeknMnnMqNIAinlpz+FoiJ44IGoIxGRmNOaxVEZNAgGD4Zf/ELrGotIpJQIonT//X4Zy+9/X3MQiUhklAiidMIJcNNN8OKL8NRTUUcjIjGlRBC1KVOgb1+YNg127Yo6GhGJISWCqDVs6KebKCyEX/866mhEJIaUCOqD00+HUaP8ILP9+6OORkRiRomgvrj6avjkE/jDH6KORERiRomgvsjLg1NO8TOTFhdHHY2IxEhoicDMOpvZIjMrMLN3zGxqOWWGmNl2M1sZbJVNe53a0tLg5z+Hf/zDT0EhIlJHKl2PoJaKgR8751aYWSaw3MwWOufeLVPuVefcuSHGkTzGjoUf/tB3Go8Z4weciYiELLQagXNuk3NuRfB4J1AAdAzr/VLGrbfC177mE8KBA1FHIyIxUCd9BGaWDZwCLC3n5YFmtsrM5ptZrwp+f7KZ5ZtZflFRUYiR1gNNm/q1Ct55B+6+O+poRCQGKp19NCFvYNYMWAz8t3PuqTKvNQcOOed2mdlo4E7n3AmVHS9lZh+tjHNwzjmwZAmsXQsdOkQdkYgkuRrPPpqAN04H5gKPlk0CAM65Hc65XcHj54F0M2sdZkxJwQxmzYJ9+/yKZiIiIQrzriEDHgQKnHPlDpk1s/ZBOczstCCerWHFlFS6d4ef/AQefRReeSXqaEQkhYXWNGRmg4BXgTXAoWD3T4EuAM65e83s34Ep+DuM9gDXOuder+y4sWgaKvHFF9CzJzRrBsuXQ6NGUUckIkmqsqah0G4fdc4twa9kVlmZuwH1iFakaVPfYXzeeXDHHRpfICKh0Mji+u7cc/08RLfe6tc5FhFJMCWCZDBzJmzbBr/6VdSRiEgKUiJIBrm5MG6cH3G8eXPU0YhIilEiSBb//d+wZw9cd13UkYhIilEiSBYnnggzZvhpqh9/POpoRCSFKBEkkxkzYMAAv7zlxo1RRyMiKUKJIJk0bAiPPOIno7v8cjh06Oi/IyJyFEoEyaZbNz8p3aJFcPvtUUcjIilAiSAZfec7fu2Cn/3Mz1IqIlILSgTJyAzuuw+aN4eLL/aT04mI1JASQbJq1w4efBBWr4bbbos6GhFJYkoEyey882D8eLjpJli/PupoRCRJKREkuzvvhIwMf0tpyIsMiUhqUiJIdscdB7fcAn/9K9x1V9TRiEgSUiJIBVdd5Wcpvf56WLUq6mhEJMkoEaQCM3joIcjKggkT/II2IiJVpESQKtq0gd//Ht57D6ZNizoaEUkiSgSpZPhwv4rZ/ffDb38bdTQikiRCW6pSInLTTbBmDVxzDXTu7PsOREQqoRpBqmnQAB57DPr1gwsugPz8qCMSkXpOiSAVNW8Of/mLH308Zgxs2BB1RCJSjykRpKq2bWHePL+q2ejR/qeISDmUCFJZ797wxBNQUAAXXqjJ6USkXEoEqW7ECPjNb3ztYORI1QxE5AhKBHEwZYpf63jxYrjkEti/P+qIRKQeUSKIi4kT/YpmTz8NQ4bAp59GHZGI1BNKBHEydarvM1i5Er71Ldi7N+qIRKQeCC0RmFlnM1tkZgVm9o6ZTS2njJnZLDNbb2arzSwnrHgkcOGF8LvfwZtvwrXXRh2NiNQDYdYIioEfO+d6AAOAH5pZzzJlRgEnBNtkQPMi1IULL4Qf/9hPQ/H881FHIyIRCy0ROOc2OedWBI93AgVAxzLFxgK/d96bQEsz6xBWTFLKzJnQty9cdJFWNxOJuTrpIzCzbOAUYGmZlzoCG0s9L+TIZIGZTTazfDPLLyoqCivMeGnaFJ59Fho2hLPPhk2boo5IRCISeiIws2bAXGCac25H2ZfL+ZUj1lt0zs12zuU653LbtGkTRpjx1KULPPccFBXB6afD8uVRRyQiEQg1EZhZOj4JPOqce6qcIoVA51LPOwEfhxmTlHHGGfDyy3DgAOTmwne/C9u2RR2ViNShMO8aMuBBoMA59+sKij0LXBbcPTQA2O6cUxtFXTvtNFi9Gq67zq90dvzxftI6EYmFMGsEZwCXAkPNbGWwjTazq8zsqqDM88AHwHrgfuAHIcYjlWnVCm67zY8x6NbNz1o6ZozWQBaJAXPuiCb5ei03N9fla479cO3YAf/zP/DAA/DZZ34g2syZ0KxZ1JGJSA2Z2XLnXG55r2lksRypeXO4+Wa//vHkyX5qij594PXXo45MREKgRCAVa93aDzp79VU4eNB3LH/ve/D551FHJiIJpEQgRzdokO87KOlM7tkTbr0Vdu6MOjIRSQAlAqmaY4/1nclLl0LnznDDDXDSSX5660OHoo5ORGpBiUCqJzfXJ4M33oDjjoPLLoOvfx3uvdePRRCRpKNEIDUzYIBPCI895m89nTIFcnLgnXeijkxEqkmJQGouLQ0mTPAJ4c9/hs2bfTKYNAnWro06OhGpIiUCqT0zGDvWDz678kpfSzjpJDj/fPjww6ijE5GjUCKQxGnf3vcV/POfMGMGvPgi9OgBN94IW7dGHZ2IVECJQBKvfXv4r/+Ct9/201TcfLMfk/Ctb8G6dVFHJyJlKBFIeLKz4fHH4bXX/O2mCxf6O4zy8vwkdyJSLygRSPhOP93XCtav901Gy5f7TuWLL4ZXXok6OpHYUyKQutOhg28y+sc/4N//HebPh8GD4frrYf/+qKMTiS3NPirR2b0bfvQjuP9+6N4dLr3U1xSaNfOznjZr5jue09N9Z3NaGnTqBAMH+tHNIlJllc0+2rCugxE57JhjYPZsf+vpjBnw859X7ffS0mDkSJ80wM+M2q+fTyZW3uqnIlIZJQKJ3jnn+G33blixArZsgcxMXwvo2RMaNPBzHaWl+buOXnjB1yKef/6rx2nTxtcW+vTxi+uMGgXt2kVzTiJJRE1Dkrz27/fzGxUUQH6+vzvp9dfhgw++LHPKKX767HHj4MwzfTIRiaHKmoaUCCT17Nvnk8NTT8GSJX4KjC++8P0LP/mJH8+gPgaJGSUCibcvvoBnnoG77vKzpppB375+1PPatb4ZqlcvGDIEzjsPGjeOOmKRhNNSlRJvTZv6yfGWLIG33oKf/cz3Obz5pu+w3r/fr6tw4YV+VPQ3vwlz50KSfUkSqSl1Fkt8pKX59RRyy/lSdPAgvPSSHwn9xhswfrxfmW3KFF9z6NoVWrTQXUmSkpQIRMDfmTRihN+Ki2HOHPiP/4BLLvmyTJMmftqMAQPgiit857NIClAfgUhF9u+H996Dd9+FwkL4+GM/Tcarr8Lnn/t+hUGD/IjpwYP9HUotWkQdtUi5NKBMpCYyMvyYhD59vrp/zx4/EO7JJ/2dSVu2+P6EJk38FBrXXqsmJEkqqhGI1NaOHX7yvAce8HcntWjhO52HDIGrr/Y1h7p28KBfKGjLFj8F+Mkn+6k6JLZ0+6hIXXAOfv1r/wG8Y4dfmGfPHj9q+oor/NoMGRnhvPeuXbBokZ/I77XXfHNWcfGXrzdu7JNB+/a+hnPiiV9uas6KBSUCkSgUFcE99/hmpE2bICvLL9954YW+ttCwkpbZ/fv9mg1vvAHbt/umpoICeP99aNUKjjvOJ5mDB2HbNl9u507/gf+Nb/ipNk480d/t9OmnflDda6/5sh984H+vRPv2fmnRQYP8RH/bt/vjfPQRbNzo93Xt6ud3GjRI4yySlBKBSJQOHvSL8syZA/Pm+QFu6ekwfLgfs9CihZ80r18//4H98MN+HMOuXV89TufOfmK9zz7zHddNm/pv/S1b+t+94gq/9kOTJpXHs3+/Tyhr1365LV8Oa9b4Wk1aGhw6BG3b+vfcsePLleWaNPFJ4cQToX9/H0/Hjr75q+T2WvWP1EuRJAIzewg4F9jsnOtdzutDgGeAfwa7nnLOzTzacZUIJKnt2eMny3vtNXj6adiw4cgyLVr4GVlHjfLf7NPT/QC4o33A19aBA7B3r08wBw589Zv/zp2weDH85S++dvPuu0cuO5qe7pPC178OJ5wAxx8PQ4f6pBF27FHYtcsPSISkSH5RJYIzgV3A7ytJBNc5586tznGVCCRlOOebYXbt8jWBggJ/K+rFFyfHB+eOHb5m8cknftbYbdv8848+8s1P27f7cmlpMGwY9O7tB+cNG+aTRHnef9/fnrt5sz/O9u2+1jFokJ+Jtl07/6H7wQd+27vXN2299ZavxbRp45NP69ZV+3A+dAj+9S9/LQ4c8M1omzb5GtfGjf64n33ma3WZmf6cd+3ytw9v3+6vU4MGvmY3aZIfY3LccfUyMURy+6hz7hUzyw7r+CJJz8w367Rs6SfESzbNm/uxE+BrL2V9/DE895yvOTz3nK8FffGFP+9vfhPOOss3ZX3xBaxc6Wscr776Zf/Fscf6/olHHvnymMce6z+8S5JMRTIy/FTkZ5/tE8/IkV921BcV+ZrNE0/4DvZ9+8o/Rnq6n5Oqc2ffn7Nzp3//rCxfa8vK8oljzx5/vPnz/e+1a+ff+6yz/DZoEDRqVPW/awRC7SMIEsG8SmoEc4FC4GN87eCdCo4zGZgM0KVLl/4ffvhhSBGLSGic88uUPvaY/+BcseKr8zmdfLL/0P7+930TU2am3//JJ77j/L334O23/Ydyt26+TNu2/hv8tm2+Oe3jj33CKSry/R6vveY/qNu18yvgvf8+/PnP/n2zs/2dXD16+A/9hg19uQ4d/Na6ddWnLT9wwNfqVqzwtYi//c0PQgR/jB49fCd+t24+eQ4c6JNcZTcMJFhkncVHSQTNgUPOuV1mNhq40zl3wtGOqaYhkRSxfbvvRM/IgFNP9R++ibZ7t++gf+QR/zMjA6ZO9XNJnXpquE04mzf7iQ2XLfPbypU+QZVo1sw3JfXv75vNTjrJJ4m0NJ+8mjb15bZs8bWqTZt88ipZma+a6mUiKKfsBiDXObelsnJKBCJSIx9/7Jt0Sjp4o7Bli08OK1f6D/cVK/xdWwcO+NcbNfK1lf37fW2heXOfzEqar6ZNg9tvr9Fb18spJsysPfCpc86Z2Wn4KbG3RhWPiKS4446LOgLf3HTuuX4rceCAn8NqxQr4+99953Nmpk8An37qHw8e7GtMZac7SZDQEoGZ/REYArQ2s0Lg50A6gHPuXmA8MMXMioE9wEUu2QY1iIjUVnq670Po0eOrs93WoTDvGppwlNfvBu4O6/1FRKRqtEKZiEjMKRGIiMScEoGISMwpEYiIxJwSgYhIzCkRiIjEnBKBiEjMJd3CNGZWBNR01rnWQKVTWKQgnXM86JzjoTbn/DXnXJvyXki6RFAbZpZf0VwbqUrnHA8653gI65zVNCQiEnNKBCIiMRe3RDA76gAioHOOB51zPIRyzrHqIxARkSPFrUYgIiJlKBGIiMRcbBKBmY00s7Vmtt7MpkcdT02ZWWczW2RmBWb2jplNDfa3MrOFZrYu+HlssN/MbFZw3qvNLKfUsS4Pyq8zs8ujOqeqMrMGZvZ3M5sXPO9qZkuD+J8ws4xgf6Pg+frg9exSx7gx2L/WzPKiOZOqMbOWZvakmYAfuVQAAAVwSURBVL0XXO+BqX6dzexHwb/rt83sj2bWONWus5k9ZGabzeztUvsSdl3NrL+ZrQl+Z5ZZFRZmds6l/AY0AN4HjgcygFVAz6jjquG5dABygseZwD+AnsCtwPRg/3TgluDxaGA+YMAAYGmwvxXwQfDz2ODxsVGf31HO/VrgMfw62AB/wq9sB3AvMCV4/APg3uDxRcATweOewbVvBHQN/k00iPq8Kjnfh4HvBo8zgJapfJ2BjsA/gSalru8VqXadgTOBHODtUvsSdl2BZcDA4HfmA6OOGlPUf5Q6+sMPBBaUen4jcGPUcSXo3J4BzgbWAh2CfR2AtcHj+4AJpcqvDV6fANxXav9XytW3DegEvAQMBeYF/8i3AA3LXmNgATAweNwwKGdlr3vpcvVtA5oHH4pWZn/KXucgEWwMPtwaBtc5LxWvM5BdJhEk5LoGr71Xav9XylW0xaVpqOQfWInCYF9SC6rCpwBLgXbOuU0Awc+2QbGKzj3Z/iZ3AD8BDgXPs4DPnXPFwfPS8R8+t+D17UH5ZDrn44EiYE7QHPaAmR1DCl9n59y/gP8FPgI24a/bclL7OpdI1HXtGDwuu79ScUkE5bWRJfV9s2bWDJgLTHPO7aisaDn7XCX76x0zOxfY7JxbXnp3OUXdUV5LmnPGf8PNAX7rnDsF2I1vMqhI0p9z0C4+Ft+ccxxwDDCqnKKpdJ2PprrnWKNzj0siKAQ6l3reCfg4olhqzczS8UngUefcU8HuT82sQ/B6B2BzsL+ic0+mv8kZwBgz2wA8jm8eugNoaWYNgzKl4z98bsHrLYBtJNc5FwKFzrmlwfMn8Ykhla/zcOCfzrki59wB4CngdFL7OpdI1HUtDB6X3V+puCSCt4ATgrsPMvAdS89GHFONBHcAPAgUOOd+XeqlZ4GSOwcux/cdlOy/LLj7YACwPah6LgBGmNmxwTexEcG+esc5d6NzrpNzLht/7V52zl0CLALGB8XKnnPJ32J8UN4F+y8K7jbpCpyA71ird5xznwAbzezEYNcw4F1S+Drjm4QGmFnT4N95yTmn7HUuJSHXNXhtp5kNCP6Gl5U6VsWi7jSpw86Z0fg7bN4HfhZ1PLU4j0H4qt5qYGWwjca3jb4ErAt+tgrKG3BPcN5rgNxSx/oOsD7Yroz63Kp4/kP48q6h4/H/wdcD/wc0CvY3Dp6vD14/vtTv/yz4W6ylCndTRHyu/YD84Fr/GX93SEpfZ+CXwHvA28Af8Hf+pNR1Bv6I7wM5gP8GPymR1xXIDf5+7wN3U+aGg/I2TTEhIhJzcWkaEhGRCigRiIjEnBKBiEjMKRGIiMScEoGISMwpEYiUYWYHzWxlqS1hs9WaWXbpWSdF6oOGRy8iEjt7nHP9og5CpK6oRiBSRWa2wcxuMbNlwdY92P81M3spmC/+JTPrEuxvZ2ZPm9mqYDs9OFQDM7s/mHf/RTNrEtlJiaBEIFKeJmWahr5d6rUdzrnT8CM27wj23Q383jnXB3gUmBXsnwUsds71xc8T9E6w/wTgHudcL+BzYFzI5yNSKY0sFinDzHY555qVs38DMNQ590Ew8d8nzrksM9uCn0v+QLB/k3OutZkVAZ2cc/tKHSMbWOicOyF4fgOQ7py7KfwzEymfagQi1eMqeFxRmfLsK/X4IOqrk4gpEYhUz7dL/XwjePw6flZUgEuAJcHjl4ApcHi95eZ1FaRIdeibiMiRmpjZylLPX3DOldxC2sjMluK/RE0I9l0DPGRm1+NXFbsy2D8VmG1mk/Df/KfgZ50UqVfURyBSRUEfQa5zbkvUsYgkkpqGRERiTjUCEZGYU41ARCTmlAhERGJOiUBEJOaUCEREYk6JQEQk5v4ft7F3EAZjES8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "epoch = np.arange(len(all_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, all_loss, 'r', label='Training loss',)\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Random Generations\n",
    "\n",
    "Let us generate some sequences using random sequences of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " nce us my teint to our blood;\n",
      "And quest are thine well.\n",
      "\n",
      "SLARENBE:\n",
      "Yet are want, deliver my lord!\n",
      "He that be the weltome, that the defence! I said, truth! That thind\n",
      "Id say long thy lount! for a tribunts,\n",
      "Shal you fault bath this conful cour? Come, son,\n",
      "Thou made ty ungle of ither follow her; you?\n",
      "\n",
      "GLOUCESTER:\n",
      "The this well, by countentent from he, love,\n",
      "Set brush'd be out legs follows to be ded.\n",
      "\n",
      "GLOUCESCERSeN:\n",
      "Means the majesty an horse! but I'll say you should I\n",
      "Did bear you! so crise heard,  \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Choose random point in text\n",
    "p = random.randint(0, len(data)-SEQUENCE_LEN-1)\n",
    "\n",
    "# Encode Input\n",
    "inputs = [char_to_ix[ch] for ch in data[p:p + SEQUENCE_LEN]]\n",
    "\n",
    "sample_ix = sample(inputs[0], 500)\n",
    "txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "print('----\\n %s \\n----' % (txt,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GRU has been able to form words from the characters available to it and put those words together to give text. The generated sequences look very similar to shakespearean literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
