{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Network \n",
    "\n",
    "We're going to build a Convolutional Neural Network from scratch and look at the math behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What inspired Convolutional Networks?\n",
    "\n",
    "CNNs are biologically-inspired models inspired by research by D. H. Hubel and T. N. Wiesel. They proposed an explanation for the way in which mammals visually perceive the world around them using a layered architecture of neurons in the brain, and this in turn inspired engineers to attempt to develop similar pattern recognition mechanisms in computer vision.\n",
    "\n",
    "<img src=\"assets/inspire.png\">\n",
    "\n",
    "In their hypothesis, within the visual cortex, complex functional responses generated by \"complex cells\" are constructed from more simplistic responses from \"simple cells'. \n",
    "\n",
    "For instances, simple cells would respond to oriented edges etc, while complex cells will also respond to oriented edges but with a degree of spatial invariance.\n",
    "\n",
    "Receptive fields exist for cells, where a cell responds to a summation of inputs from other local cells.\n",
    "\n",
    "The architecture of deep convolutional neural networks was inspired by the ideas mentioned above \n",
    "- local connections \n",
    "- layering  \n",
    "- spatial invariance (shifting the input signal results in an equally shifted output signal. , most of us are able to recognize specific faces under a variety of conditions because we learn abstraction These abstractions are thus invariant to size, contrast, rotation, orientation\n",
    " \n",
    "However, it remains to be seen if these computational mechanisms of convolutional neural networks are similar to the computation mechanisms occurring in the primate visual system\n",
    "\n",
    "- convolution operation\n",
    "- shared weights\n",
    "- pooling/subsampling \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works ?\n",
    "\n",
    "<img src=\"assets/cnn_model.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "from functools import reduce\n",
    "import struct\n",
    "from glob import glob\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Prepare a dataset of images\n",
    "\n",
    "<img src=\"assets/image.png\">\n",
    "\n",
    "- Every image is a matrix of pixel values. \n",
    "- The range of values that can be encoded in each pixel depends upon its bit size. \n",
    "- Most commonly, we have 8 bit or 1 Byte-sized pixels. Thus the possible range of values a single pixel can represent is [0, 255]. \n",
    "- However, with coloured images, particularly RGB (Red, Green, Blue)-based images, the presence of separate colour channels (3 in the case of RGB images) introduces an additional ‘depth’ field to the data, making the input 3-dimensional. \n",
    "- Hence, for a given RGB image of size, say 255×255 (Width x Height) pixels, we’ll have 3 matrices associated with each image, one for each of the colour channels. \n",
    "- Thus the image in it’s entirety, constitutes a 3-dimensional structure called the Input Volume (255x255x3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    images_path = glob('./%s/%s*3-ubyte' % (path, kind))[0]\n",
    "    labels_path = glob('./%s/%s*1-ubyte' % (path, kind))[0]\n",
    "\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II',\n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath,\n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack('>IIII',\n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath,\n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_mnist('./data/mnist')\n",
    "test_images, test_labels = load_mnist('./data/mnist', 't10k')\n",
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us look at the result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2- Define Helper Class\n",
    "\n",
    "Before creating the Neural Network we will have to define a helper class AverageMeter which will keep track losses and accuracies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Define the Convolutional Layer \n",
    "\n",
    "![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/more_images/Convolution_schematic.gif \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](http://xrds.acm.org/blog/wp-content/uploads/2016/06/Figure_2.png \"Logo Title Text 1\")\n",
    "\n",
    "- A convolution is an orderly procedure where two sources of information are intertwined.\n",
    "\n",
    "- A kernel (also called a filter) is a smaller-sized matrix in comparison to the input dimensions of the image, that consists of real valued entries.\n",
    "\n",
    "- Kernels are then convolved with the input volume to obtain so-called ‘activation maps’ (also called feature maps).  \n",
    "- Activation maps indicate ‘activated’ regions, i.e. regions where features specific to the kernel have been detected in the input. \n",
    "\n",
    "- The real values of the kernel matrix change with each learning iteration over the training set, indicating that the network is learning to identify which regions are of significance for extracting features from the data.\n",
    "\n",
    "- We compute the dot product between the kernel and the input matrix. -The convolved value obtained by summing the resultant terms from the dot product forms a single entry in the activation matrix. \n",
    "\n",
    "- The patch selection is then slided (towards the right, or downwards when the boundary of the matrix is reached) by a certain amount called the ‘stride’ value, and the process is repeated till the entire input image has been processed. - The process is carried out for all colour channels.\n",
    "\n",
    "- instead of connecting each neuron to all possible pixels, we specify a 2 dimensional region called the ‘receptive field[14]’ (say of size 5×5 units) extending to the entire depth of the input (5x5x3 for a 3 colour channel input), within which the encompassed pixels are fully connected to the neural network’s input layer. It’s over these small regions that the network layer cross-sections (each consisting of several neurons (called ‘depth columns’)) operate and produce the activation map. (reduces computational complexity)\n",
    "\n",
    "![alt text](http://i.imgur.com/g4hRI6Z.png \"Logo Title Text 1\")\n",
    "![alt text](http://i.imgur.com/tpQvMps.jpg \"Logo Title Text 1\")\n",
    "![alt text](http://i.imgur.com/oyXkhHi.jpg \"Logo Title Text 1\")\n",
    "![alt text](http://xrds.acm.org/blog/wp-content/uploads/2016/06/Figure_5.png \"Logo Title Text 1\")\n",
    "\n",
    "Great resource on description of  convolution (discrete vs continous)  & the fourier transform\n",
    "\n",
    "http://timdettmers.com/2015/03/26/convolution-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(object):\n",
    "    def __init__(self, shape, output_channels, ksize, stride=1, padding=1, weights=None):\n",
    "        assert len(shape)==4\n",
    "        assert isinstance(ksize, int) or len(ksize)==2\n",
    "        self.inp_shape = shape\n",
    "        self.output_channels = output_channels\n",
    "        self.padding = padding\n",
    "        self.input_channels = shape[1]\n",
    "        self.input_height = shape[2]\n",
    "        self.input_width = shape[3]\n",
    "        self.bs = shape[0]\n",
    "        self.stride = stride\n",
    "        self.ksize = (ksize,ksize) if isinstance(ksize, int) else ksize\n",
    "        assert (self.input_height-self.ksize[0]+2*padding)%stride==0\n",
    "        assert (self.input_width-self.ksize[1]+2*padding)%stride==0\n",
    "        self.output_height = (self.input_height-self.ksize[0]+2*padding)//stride+1\n",
    "        self.output_width = (self.input_width-self.ksize[1]+2*padding)//stride+1\n",
    "\n",
    "        # self.weights = np.random.standard_normal((self.output_channels, self.input_channels,\n",
    "        #                                           ksize[0], ksize[1]))\n",
    "        # self.bias = np.random.standard_normal(self.output_channels)\n",
    "        fan_in = self.input_channels*self.ksize[0]*self.ksize[1]\n",
    "        self.weights = np.random.standard_normal((self.output_channels, self.input_channels, self.ksize[0], self.ksize[1]))/np.sqrt(fan_in/2)\n",
    "        self.bias= np.zeros(self.output_channels)\n",
    "\n",
    "        self.delta = np.zeros((self.bs, self.output_channels, self.output_height, self.output_width))\n",
    "\n",
    "        self.w_gradient = np.zeros_like(self.weights)\n",
    "        self.b_gradient = np.zeros_like(self.bias)\n",
    "        self.weights_m = np.zeros_like(self.weights)\n",
    "        self.bias_m = np.zeros_like(self.bias)\n",
    "\n",
    "        self.output_shape = self.delta.shape\n",
    "        self.i, self.j, self.k = self.get_im2col_slice(x_shape=shape, ksize=self.ksize)\n",
    "\n",
    "    def forward(self, x, w=None, b=None):\n",
    "        '''\n",
    "        convolution forward\n",
    "        :param x: input, shape is [batch_size, channels, height, width]\n",
    "        :return:\n",
    "        '''\n",
    "        if w is not None:\n",
    "            self.weights = w\n",
    "        if b is not None:\n",
    "            self.bias = b\n",
    "        weight_cols = self.weights.reshape(self.output_channels, -1)  # 64,27\n",
    "        self.x_pad = x\n",
    "        if self.padding!=0:\n",
    "            self.x_pad = np.pad(x, ((0,0), (0,0),(self.padding, self.padding),\n",
    "                                    (self.padding, self.padding)), mode=\"constant\")\n",
    "        img_cols = self.x_pad[:,self.k, self.i, self.j]\n",
    "        img_cols = img_cols.transpose(1,0,2)\n",
    "        img_cols = img_cols.reshape(img_cols.shape[0], -1)  # 27,50176\n",
    "        out = np.dot(weight_cols, img_cols).transpose(1,0)+self.bias    # 50176,64\n",
    "        out = out.reshape(self.bs, self.output_height, self.output_width, -1).transpose(0,3,1,2)\n",
    "\n",
    "        cache = (x, self.weights, self.bias, img_cols)\n",
    "        return out, cache\n",
    "\n",
    "    def gradient(self, delta, cache):\n",
    "        x, self.weights, self.bias, img_cols = cache\n",
    "        stride = self.stride\n",
    "        padding = self.padding\n",
    "\n",
    "        delta_reshape = delta.transpose(1,0,2,3)\n",
    "        delta_reshape = delta_reshape.reshape(delta_reshape.shape[0], -1)\n",
    "        self.w_gradient = np.dot(delta_reshape, img_cols.T).reshape(self.w_gradient.shape)\n",
    "        self.b_gradient = np.sum(delta_reshape,axis=1)\n",
    "\n",
    "        # backward to get the previous layer's delta\n",
    "        num_filters, _, f_h, f_w = self.weights.shape\n",
    "        dx_cols = self.weights.reshape(num_filters, -1).T.dot(delta_reshape)\n",
    "        dx = self.col2im(dx_cols, x.shape, ksize=self.ksize)\n",
    "\n",
    "        # delta_pad = delta\n",
    "        # if self.padding!=0:\n",
    "        #     delta_pad = np.pad(delta, ((0,0), (0,0), (self.padding, self.padding),\n",
    "        #                        (self.padding, self.padding)), mode=\"constant\")\n",
    "        # delta_cols = delta_pad[:,self.k, self.i, self.j]    # 4,27,50176\n",
    "        # delta_cols = delta_cols.transpose(1,0,2)\n",
    "        # delta_cols = delta_cols.reshape(delta_cols.shape[0], -1) #27,200704\n",
    "        # fliped_weight = np.flip(np.flip(self.weights, axis=2), axis=3)\n",
    "        # fliped_weight_cols = fliped_weight.reshape(self.output_channels, -1)  # 64,27\n",
    "        # prev_delta = np.dot(fliped_weight_cols, delta_cols)    # 64,200704\n",
    "        # prev_delta = prev_delta.reshape()\n",
    "\n",
    "        return self.w_gradient, self.b_gradient, dx\n",
    "\n",
    "\n",
    "    def get_im2col_slice(self, x_shape, ksize):\n",
    "        k_h, k_w = ksize\n",
    "        bs, c, i_h, i_w = x_shape\n",
    "        i0 = np.repeat(np.arange(k_h), k_w)\n",
    "        i0 = np.tile(i0, c)\n",
    "        i1 = self.stride*np.repeat(np.arange(self.output_height), self.output_width)\n",
    "        i= i0.reshape(-1,1)+i1.reshape(1,-1)\n",
    "\n",
    "        j0 = np.tile(np.arange(k_w), k_h*c)\n",
    "        j1 = self.stride*np.tile(np.arange(self.output_width), self.output_height)\n",
    "        j=j0.reshape(-1,1)+j1.reshape(1,-1)\n",
    "\n",
    "        k = np.repeat(np.arange(c), k_w*k_h).reshape(-1,1)\n",
    "\n",
    "        return i,j,k\n",
    "\n",
    "    def col2im(self, cols, x_shape, ksize):\n",
    "\n",
    "        padding = self.padding\n",
    "        N, C, H, W = x_shape\n",
    "        field_height, field_width = ksize\n",
    "        H_padded, W_padded = H + 2 * padding, W + 2 * padding\n",
    "        x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n",
    "\n",
    "        i,j,k = self.get_im2col_slice(x_shape, (field_height, field_width))\n",
    "        cols_reshaped = cols.T.reshape(N, C * field_height * field_width, -1)\n",
    "        # cols_reshaped = cols_reshaped.transpose(2, 1, 0)\n",
    "        np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n",
    "        if padding == 0:\n",
    "            return x_padded\n",
    "\n",
    "        return x_padded[:, :, padding:-padding, padding:-padding]\n",
    "\n",
    "    def backward(self, alpha=1e-4, wd=4e-4, momentum=0.9):\n",
    "        self.weights *= (1 - wd)\n",
    "        self.bias *= (1 - wd)\n",
    "        self.weights_m = momentum*self.weights_m-alpha*self.w_gradient\n",
    "        self.bias_m = momentum*self.bias_m-alpha*self.b_gradient\n",
    "        self.weights += self.weights_m\n",
    "        self.bias += self.bias_m\n",
    "\n",
    "        self.w_gradient = np.zeros_like(self.w_gradient)\n",
    "        self.b_gradient = np.zeros_like(self.b_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Define the Pooling Layer\n",
    "\n",
    "![alt text](http://xrds.acm.org/blog/wp-content/uploads/2016/06/Figure_6.png \"Logo Title Text 1\")\n",
    "\n",
    "- Pooling reducing the spatial dimensions (Width x Height) of the Input Volume for the next Convolutional Layer. It does not affect the depth dimension of the Volume.  \n",
    "- The transformation is either performed by taking the maximum value from the values observable in the window (called ‘max pooling’), or by taking the average of the values. Max pooling has been favoured over others due to its better performance characteristics.\n",
    "- also called downsampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool(object):\n",
    "    def __init__(self, pool_h, pool_w, stride):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape)==4\n",
    "        (N,C,H,W) = x.shape\n",
    "        pool_h = self.pool_h\n",
    "        pool_w = self.pool_w\n",
    "        stride = self.stride\n",
    "        H_prime = (H-pool_h)//stride+1\n",
    "        W_prime = (W-pool_w)//stride+1\n",
    "\n",
    "        out = np.zeros((N,C,H_prime,W_prime))\n",
    "\n",
    "        for n in range(N):\n",
    "            for h in range(H_prime):\n",
    "                for w in range(W_prime):\n",
    "                    h1 = h*stride\n",
    "                    h2 = h1 + pool_h\n",
    "                    w1 = w*stride\n",
    "                    w2 = w1 + pool_w\n",
    "                    window = x[n,:,h1:h2, w1:w2]\n",
    "                    out[n,:,h,w] = np.max(np.reshape(window, newshape=(C, -1)), axis=1)\n",
    "\n",
    "        cache = x\n",
    "        return out, cache\n",
    "\n",
    "    def gradient(self, dout, cache):\n",
    "        x = cache\n",
    "        assert len(x.shape)==4\n",
    "        (N,C,H,W) = x.shape\n",
    "        pool_h = self.pool_h\n",
    "        pool_w = self.pool_w\n",
    "        stride = self.stride\n",
    "        H_prime = (H-pool_h)//stride+1\n",
    "        W_prime = (W-pool_w)//stride+1\n",
    "\n",
    "        dx = np.zeros_like(x)\n",
    "\n",
    "        for n in range(N):\n",
    "            for c in range(C):\n",
    "                for h in range(H_prime):\n",
    "                    for w in range(W_prime):\n",
    "                        h1 = h*stride\n",
    "                        h2 = h1+pool_h\n",
    "                        w1 = w*stride\n",
    "                        w2 = w1+pool_w\n",
    "                        window1 = x[n,c,h1:h2, w1:w2].reshape(-1)\n",
    "                        window2 = np.zeros_like(window1)\n",
    "                        window2[np.argmax(window1)]=1\n",
    "                        window3 = window2.reshape(pool_h, pool_w)\n",
    "\n",
    "                        dx[n,c,h1:h2,w1:w2] = window3*dout[n,c,h,w]\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Define Dropout, Activation and Batch Norm Layers\n",
    "\n",
    "Dropout is a technique where randomly selected neurons are ignored during training. They are “dropped-out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass.\n",
    "![alt text](https://miro.medium.com/max/1026/1*dEi_IkVB7IpkzZ-6H0Vpsg.png)\n",
    "\n",
    "Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. We use Batch Normalization for two reasons:\n",
    "- We can use higher learning rates because batch normalization makes sure that there’s no activation that’s gone really high or really low. And by that, things that previously couldn’t get to train, it will start to train.\n",
    "\n",
    "- It reduces overfitting because it has a slight regularization effects. Similar to dropout, it adds some noise to each hidden layer’s activations. Therefore, if we use batch normalization, we will use less dropout, which is a good thing because we are not going to lose a lot of information. However, we should not depend only on batch normalization for regularization; we should better use it together with dropout.\n",
    "\n",
    "For the Activation Layer we will be using ReLu due to its easier computability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout Layer to prevent Overfitting\n",
    "class Dropout(object):\n",
    "    def __init__(self, keep_rate):\n",
    "        self.keep_rate = keep_rate\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        if training:\n",
    "            mask = np.random.binomial(1, self.keep_rate, size=x.shape)/self.keep_rate\n",
    "            mask_x = x*mask\n",
    "\n",
    "            cache = mask\n",
    "            return mask_x, cache\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def gradient(self, dout, cache):\n",
    "        mask = cache\n",
    "        dx = dout*mask\n",
    "        return dx\n",
    "\n",
    "class ReLU(object):\n",
    "    def forward(self, x):\n",
    "        out = np.maximum(0, x)\n",
    "        cache = x\n",
    "        return out, cache\n",
    "\n",
    "    def gradient(self, dout, cache):\n",
    "        x = cache\n",
    "        dx = np.array(dout)\n",
    "        dx[x<=0] = 0\n",
    "        return dx    \n",
    "    \n",
    "# Normalization for Convolutional Layers\n",
    "class BatchNorm2D(object):\n",
    "    def __init__(self, num_features, affine=True, moving_decay=0.9, epsilon=1e-8):\n",
    "        '''\n",
    "        Apply to convolution layer\n",
    "        :param num_features:\n",
    "        :param affine:\n",
    "        :param moving_decay:\n",
    "        :param epsilon:\n",
    "        '''\n",
    "        self.num_features = num_features\n",
    "        self.affine = affine\n",
    "        self.epsilon = epsilon\n",
    "        self.moving_decay = moving_decay\n",
    "        if self.affine:\n",
    "            self.weight = np.ones((num_features,))\n",
    "            self.bias = np.zeros((num_features,))\n",
    "        self.moving_mean = np.zeros((num_features,))\n",
    "        self.moving_var = np.ones((num_features,))\n",
    "        self.mean = np.zeros((num_features,))\n",
    "        self.var = np.ones((num_features,))\n",
    "        self.weights_m = np.zeros_like(self.weight)\n",
    "        self.bias_m = np.zeros_like(self.bias)\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        assert len(x.shape)==4\n",
    "        B = x.shape[0]\n",
    "        mean = np.mean(x, axis=(0,2,3))\n",
    "        var = np.var(x, axis=(0,2,3))\n",
    "        self.mean = mean\n",
    "        self.var = var\n",
    "        if training:\n",
    "            if np.mean(self.moving_mean)==0.0 and np.mean(self.moving_var)==1.0:\n",
    "                self.moving_mean = mean\n",
    "                self.moving_var = var\n",
    "            else:\n",
    "                self.moving_mean = self.moving_mean*self.moving_decay+(1-self.moving_decay)*mean\n",
    "                self.moving_var = self.moving_var*self.moving_decay+(1-self.moving_decay)*var\n",
    "            normx = (x - mean[np.newaxis,:,np.newaxis,np.newaxis])/np.sqrt(var[np.newaxis,:,np.newaxis,np.newaxis]+self.epsilon)\n",
    "        else:\n",
    "            normx = (x - self.moving_mean[np.newaxis,:,np.newaxis,np.newaxis])/np.sqrt(self.moving_var[np.newaxis,:,np.newaxis,np.newaxis]+self.epsilon)\n",
    "\n",
    "        cache = (x, normx)\n",
    "\n",
    "        return normx*self.weight[np.newaxis,:,np.newaxis,np.newaxis]+self.bias[np.newaxis,:,np.newaxis,np.newaxis], cache\n",
    "\n",
    "    def gradient(self, dout, cache):\n",
    "        x, normx = cache\n",
    "        B = x.shape[0]*x.shape[2]*x.shape[3]\n",
    "        mean = self.mean\n",
    "        var = self.var\n",
    "        x_mu = x-mean[np.newaxis,:,np.newaxis,np.newaxis]\n",
    "\n",
    "        d_normx = dout*self.weight[np.newaxis,:,np.newaxis,np.newaxis]\n",
    "        d_var = np.sum(d_normx*x_mu, axis=(0,2,3))*-.5*(var+self.epsilon)**(-3/2)\n",
    "        d_mu = np.sum(d_normx*-1/np.sqrt(var+self.epsilon)[np.newaxis,:,np.newaxis,np.newaxis], axis=(0,2,3))+d_var*np.sum(-2*x_mu,axis=(0,2,3))/B\n",
    "        d_x = d_normx/np.sqrt(var+self.epsilon)[np.newaxis,:,np.newaxis,np.newaxis]+\\\n",
    "              d_var[np.newaxis,:,np.newaxis,np.newaxis]*2*x_mu/B+\\\n",
    "              d_mu[np.newaxis,:,np.newaxis,np.newaxis]/B\n",
    "        self.d_weight = np.sum(dout*normx, axis=(0,2,3))\n",
    "        self.d_bias = np.sum(dout, axis=(0,2,3))\n",
    "\n",
    "        return self.d_weight, self.d_bias, d_x\n",
    "\n",
    "    def backward(self, alpha=1e-4, momentum=0.9):\n",
    "        self.weights_m = momentum * self.weights_m - alpha * self.d_weight\n",
    "        self.bias_m = momentum * self.bias_m - alpha * self.d_bias\n",
    "        self.weight += self.weights_m\n",
    "        self.bias += self.bias_m\n",
    "\n",
    "        self.d_weight = np.zeros_like(self.d_weight)\n",
    "        self.d_bias = np.zeros_like(self.d_bias)\n",
    "\n",
    "# Normalization for Linear layer\n",
    "class BatchNorm1D(object):\n",
    "    def __init__(self, num_features, affine=True, moving_decay=0.99, epsilon=1e-8):\n",
    "        '''\n",
    "        Apply to the linear layer\n",
    "        :param num_features:\n",
    "        :param affine:\n",
    "        :param moving_decay:\n",
    "        :param epsilon:\n",
    "        '''\n",
    "        self.num_features = num_features\n",
    "        self.affine = affine\n",
    "        self.moving_decay = moving_decay\n",
    "        self.epsilon = epsilon\n",
    "        if self.affine:\n",
    "            self.weight = np.ones((num_features,))\n",
    "            self.bias = np.zeros((num_features,))\n",
    "        self.moving_mean = np.zeros((num_features,))\n",
    "        self.moving_var = np.ones((num_features,))\n",
    "        self.mean = np.zeros((num_features,))\n",
    "        self.var = np.ones((num_features,))\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        assert len(x.shape)==2\n",
    "        B = x.shape[0]\n",
    "        mean = np.mean(x, axis=0)\n",
    "        var = np.var(x, axis=0)\n",
    "        self.mean = mean\n",
    "        self.var = var\n",
    "        if training:\n",
    "            if np.mean(self.moving_mean) == 0.0 and np.mean(self.moving_var) == 1.0:\n",
    "                self.moving_mean = mean\n",
    "                self.moving_var = var\n",
    "            else:\n",
    "                self.moving_mean = self.moving_mean * self.moving_decay + (1 - self.moving_decay) * mean\n",
    "                self.moving_var = self.moving_var * self.moving_decay + (1 - self.moving_decay) * var\n",
    "            normx = (x - mean[np.newaxis, :]) / np.sqrt(var[np.newaxis, :] + self.epsilon)\n",
    "        else:\n",
    "            normx = (x - self.moving_mean[np.newaxis, :]) / np.sqrt(self.moving_var[np.newaxis, :] + self.epsilon)\n",
    "\n",
    "        cache = (x, normx)\n",
    "\n",
    "        return normx * self.weight[np.newaxis, :] + self.bias[np.newaxis, :], cache\n",
    "\n",
    "    def gradient(self, dout, cache):\n",
    "        x, normx = cache\n",
    "        B = x.shape[0]\n",
    "        mean = self.mean\n",
    "        var = self.var\n",
    "        x_mu = x - mean[np.newaxis, :]\n",
    "\n",
    "        d_normx = dout * self.weight[np.newaxis, :]\n",
    "        d_var = np.sum(d_normx * x_mu, axis=0) * -.5 * (var + self.epsilon) ** (-3 / 2)\n",
    "        d_mu = np.sum(d_normx * -1 / np.sqrt(var + self.epsilon)[np.newaxis, :],\n",
    "                      axis=0) + d_var * np.sum(-2 * x_mu, axis=0) / B\n",
    "        d_x = d_normx / np.sqrt(var + self.epsilon)[np.newaxis, :] + \\\n",
    "              d_var[np.newaxis, :] * 2 * x_mu / B + \\\n",
    "              d_mu[np.newaxis, :] / B\n",
    "        self.d_weight = np.sum(dout * normx, axis=0)\n",
    "        self.d_bias = np.sum(dout, axis=0)\n",
    "\n",
    "        return self.d_weight, self.d_bias, d_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Define the Final Layers \n",
    "\n",
    "![alt text](https://miro.medium.com/max/1400/1*IWUxuBpqn2VuV-7Ubr01ng.png)\n",
    "\n",
    "The output from the convolutional layers is passed through this layer which flattens it into a 1-dimensional array for inputting it to the next layer. We flatten the output of the convolutional layers to create a single long feature vector. And it is connected to the final classification model, which is called a fully-connected layer. In other words, we put all the pixel data in one line and make connections with the final layer which will be linear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fully Connected Layer\n",
    "class Linear(object):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.W = np.random.standard_normal((in_channels, out_channels))/np.sqrt(in_channels/2)\n",
    "        self.b = np.zeros((out_channels,))\n",
    "        self.gradient_W = np.zeros_like(self.W)\n",
    "        self.gradient_b = np.zeros_like(self.b)\n",
    "        self.weights_m = np.zeros_like(self.W)\n",
    "        self.bias_m = np.zeros_like(self.b)\n",
    "\n",
    "    def forward(self, x, w=None, b=None):\n",
    "        if w is not None:\n",
    "            self.W = w\n",
    "        if b is not None:\n",
    "            self.b = b\n",
    "        assert len(x.shape)==2\n",
    "        out = np.dot(x, self.W)+self.b\n",
    "        return out, x\n",
    "\n",
    "    def gradient(self, dout, cache):\n",
    "        x = cache\n",
    "        self.gradient_W = np.dot(x.T, dout)\n",
    "        self.gradient_b = np.sum(dout, 0)\n",
    "\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "\n",
    "        return self.gradient_W, self.gradient_b, dx\n",
    "\n",
    "    def backward(self, alpha=1e-4, wd=4e-4, momentum = 0.9):\n",
    "        self.W*=(1-wd)\n",
    "        self.b*=(1-wd)\n",
    "\n",
    "        self.weights_m = momentum * self.weights_m - alpha * self.gradient_W\n",
    "        self.bias_m = momentum * self.bias_m - alpha * self.gradient_b\n",
    "\n",
    "        self.W += self.weights_m\n",
    "        self.b += self.bias_m\n",
    "\n",
    "        self.gradient_W = np.zeros_like(self.gradient_W)\n",
    "        self.gradient_b = np.zeros_like(self.gradient_b)\n",
    "\n",
    "    \n",
    "# Final Output Layer\n",
    "class Softmax_and_Loss(object):\n",
    "    \"\"\"\n",
    "    Compute the softmax and loss together\n",
    "    \"\"\"\n",
    "    def forward_and_backward(self, x, y):\n",
    "        \"\"\"\n",
    "        forward and backward\n",
    "        :param x: final layer output, before softmax layer, [N,C]\n",
    "        :param y: ground truth, [N,]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        probs = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "        N = x.shape[0]\n",
    "        loss = -np.sum(np.log(probs[np.arange(N),y]))/N\n",
    "        dx = probs.copy()\n",
    "        dx[np.arange(N), y]-=1.0\n",
    "\n",
    "        dx/=N   # for the loss is divided by N\n",
    "        return loss, dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 - Define the Neural Network\n",
    "\n",
    "Now we can define a Neural Network class that can be used to combine all the layers defined previously. The class will also contain the fit function that will be used to train the network using the calculated gradients by each of the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "init_lr = 0.1\n",
    "# Define network\n",
    "conv1 = Conv2D(shape=[batch_size, 1, 32, 32], output_channels=6, ksize=5, stride=1, padding=0)    #28*28\n",
    "bn1 = BatchNorm2D(6)\n",
    "relu1 = ReLU()\n",
    "pool1 = MaxPool(2,2,2)  #14*14\n",
    "conv2 = Conv2D(shape=[batch_size, 6, 14, 14], output_channels=16, ksize=5, stride=1, padding=0)    #10*10\n",
    "bn2 = BatchNorm2D(16)\n",
    "relu2 = ReLU()\n",
    "pool2 = MaxPool(2,2,2)  # 5*5\n",
    "conv3 = Conv2D(shape=[batch_size, 16, 5, 5], output_channels=120, ksize=5, stride=1, padding=0)    #1*1\n",
    "bn3 = BatchNorm2D(120)\n",
    "relu3 = ReLU()\n",
    "fc1 = Linear(1*1*120, 84)\n",
    "relu4 = ReLU()\n",
    "dp = Dropout(0.9)\n",
    "fc2 = Linear(84, 10)\n",
    "sf = Softmax_and_Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Step - Define Variables and Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "step=0\n",
    "train_loss = AverageMeter()\n",
    "train_acc = AverageMeter()\n",
    "val_loss = AverageMeter()\n",
    "val_acc = AverageMeter()\n",
    "train_loss_all=[]\n",
    "val_loss_all=[]\n",
    "train_acc_all=[]\n",
    "val_acc_all=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0.1 3.1540872135852966 6.25\n",
      "val loss: 2.3348269844157747 val acc: 18.67988782051282\n",
      "0 1 0.1 2.4771103255357714 11.71875\n",
      "0 2 0.1 2.1328266004634946 28.125\n",
      "0 3 0.1 2.017153303372607 34.375\n",
      "0 4 0.1 1.853635223016993 41.015625\n",
      "0 5 0.1 1.7381624984098238 46.09375\n",
      "0 6 0.1 1.6132165778212546 50.911458333333336\n",
      "0 7 0.1 1.511949545977372 54.129464285714285\n",
      "0 8 0.1 1.4139728087495063 57.03125\n",
      "0 9 0.1 1.3165090180298387 60.416666666666664\n",
      "0 10 0.1 1.2413694061714469 62.5\n",
      "val loss: 1.363293542458472 val acc: 60.31650641025641\n",
      "0 11 0.1 0.436363269029711 92.1875\n",
      "0 12 0.1 0.47762266210718907 87.5\n",
      "0 13 0.1 0.49588191699575823 85.9375\n",
      "0 14 0.1 0.4701003617991034 86.1328125\n",
      "0 15 0.1 0.47244973112771305 85.625\n",
      "0 16 0.1 0.4580994433464472 86.06770833333333\n",
      "0 17 0.1 0.46187849999591685 85.82589285714286\n",
      "0 18 0.1 0.4377866734457216 86.42578125\n",
      "0 19 0.1 0.4506142937886967 86.19791666666667\n",
      "0 20 0.1 0.45158502960469155 86.09375\n",
      "val loss: 0.9891267621648621 val acc: 74.69951923076923\n",
      "0 21 0.1 0.43229119488880563 86.71875\n",
      "0 22 0.1 0.3680818933168618 88.671875\n",
      "0 23 0.1 0.38074145034628143 86.71875\n",
      "0 24 0.1 0.3843071543626904 86.71875\n",
      "0 25 0.1 0.463851679504322 86.40625\n",
      "0 26 0.1 0.4479613541552307 86.71875\n",
      "0 27 0.1 0.4285672232646129 87.05357142857143\n",
      "0 28 0.1 0.4328062563086613 87.109375\n",
      "0 29 0.09000000000000001 0.44687259820259206 86.80555555555556\n",
      "0 30 0.09000000000000001 0.44617204857366444 86.484375\n",
      "val loss: 0.5968771541359522 val acc: 82.10136217948718\n",
      "0 31 0.09000000000000001 0.6219894582659973 82.8125\n",
      "0 32 0.09000000000000001 0.4937240503349573 85.15625\n",
      "0 33 0.09000000000000001 0.43494795063455793 86.71875\n",
      "0 34 0.09000000000000001 0.39223865297509375 87.5\n",
      "0 35 0.09000000000000001 0.4217305708062267 86.5625\n",
      "0 36 0.09000000000000001 0.39928865916109646 87.36979166666667\n",
      "0 37 0.09000000000000001 0.39863417056363387 87.5\n",
      "0 38 0.09000000000000001 0.3916739453059098 87.59765625\n",
      "0 39 0.09000000000000001 0.37941009940314013 88.02083333333333\n",
      "0 40 0.09000000000000001 0.3674957747943881 88.59375\n",
      "val loss: 0.3710343924542428 val acc: 88.28125\n",
      "0 41 0.09000000000000001 0.2296661905380642 93.75\n",
      "0 42 0.09000000000000001 0.21323549046914475 92.96875\n",
      "0 43 0.09000000000000001 0.2089813253655919 93.75\n",
      "0 44 0.09000000000000001 0.20772712277269298 93.9453125\n",
      "0 45 0.09000000000000001 0.2238000180966541 94.0625\n",
      "0 46 0.09000000000000001 0.2094173873212657 94.27083333333333\n",
      "0 47 0.09000000000000001 0.19723202152744748 94.30803571428571\n",
      "0 48 0.09000000000000001 0.2161788686872069 93.65234375\n",
      "0 49 0.09000000000000001 0.21244699699734249 93.75\n",
      "0 50 0.09000000000000001 0.21334744863754943 93.75\n",
      "val loss: 0.3258492053322823 val acc: 89.1826923076923\n",
      "0 51 0.09000000000000001 0.15912956348477336 96.09375\n",
      "0 52 0.09000000000000001 0.19601133097102175 94.140625\n",
      "0 53 0.09000000000000001 0.202953995536166 94.27083333333333\n",
      "0 54 0.09000000000000001 0.19241997034726843 94.3359375\n",
      "0 55 0.09000000000000001 0.21650828921101342 93.75\n",
      "0 56 0.09000000000000001 0.2067295456157433 94.53125\n",
      "0 57 0.09000000000000001 0.2089571627462083 94.30803571428571\n",
      "0 58 0.09000000000000001 0.22492222580432297 93.9453125\n",
      "0 59 0.08100000000000002 0.229763612031757 93.75\n",
      "0 60 0.08100000000000002 0.22222442009774107 94.21875\n",
      "val loss: 0.21982279830308918 val acc: 93.12900641025641\n",
      "0 61 0.08100000000000002 0.27036186910008064 94.53125\n",
      "0 62 0.08100000000000002 0.2309644507613418 94.921875\n",
      "0 63 0.08100000000000002 0.23076336642614703 93.48958333333333\n",
      "0 64 0.08100000000000002 0.23241403994301713 93.5546875\n",
      "0 65 0.08100000000000002 0.22708420984821603 93.90625\n",
      "0 66 0.08100000000000002 0.22506798943812054 93.48958333333333\n",
      "0 67 0.08100000000000002 0.22728592003933792 93.75\n",
      "0 68 0.08100000000000002 0.21986080456262858 93.75\n",
      "0 69 0.08100000000000002 0.22588492773731947 93.48958333333333\n",
      "0 70 0.08100000000000002 0.2273321127502393 93.4375\n",
      "val loss: 0.19702426346329024 val acc: 93.95032051282051\n",
      "0 71 0.08100000000000002 0.1965940654136133 95.3125\n",
      "0 72 0.08100000000000002 0.18684738917899066 94.921875\n",
      "0 73 0.08100000000000002 0.18013007439867643 95.05208333333333\n",
      "0 74 0.08100000000000002 0.16504185270223076 95.3125\n",
      "0 75 0.08100000000000002 0.16793397199491905 95.46875\n",
      "0 76 0.08100000000000002 0.17709255651721825 95.18229166666667\n",
      "0 77 0.08100000000000002 0.17225464562265705 95.08928571428571\n",
      "0 78 0.08100000000000002 0.1683828244453448 95.1171875\n",
      "0 79 0.08100000000000002 0.16556677005814244 95.22569444444444\n",
      "0 80 0.08100000000000002 0.178504667225098 95.0\n",
      "val loss: 0.25433560326334576 val acc: 92.40785256410257\n",
      "0 81 0.08100000000000002 0.14794798703640671 94.53125\n",
      "0 82 0.08100000000000002 0.2109767491524518 94.53125\n",
      "0 83 0.08100000000000002 0.1900920919409801 95.05208333333333\n",
      "0 84 0.08100000000000002 0.19796324078210878 95.5078125\n",
      "0 85 0.08100000000000002 0.17437211874616937 95.9375\n",
      "0 86 0.08100000000000002 0.18564643319310214 95.44270833333333\n",
      "0 87 0.08100000000000002 0.1700303758774118 95.64732142857143\n",
      "0 88 0.08100000000000002 0.1863402097756461 95.01953125\n",
      "0 89 0.0729 0.18288503267542913 95.13888888888889\n",
      "0 90 0.0729 0.17853027466878169 95.234375\n",
      "val loss: 0.22857437404247596 val acc: 93.09895833333333\n",
      "0 91 0.0729 0.16339384728020656 93.75\n",
      "0 92 0.0729 0.22249534368923782 93.359375\n",
      "0 93 0.0729 0.19610295936115654 94.53125\n",
      "0 94 0.0729 0.17656288885495913 95.1171875\n",
      "0 95 0.0729 0.1758515354814028 94.84375\n",
      "0 96 0.0729 0.18595153563178957 94.79166666666667\n",
      "0 97 0.0729 0.1781501489981528 94.97767857142857\n",
      "0 98 0.0729 0.17835246773092736 94.82421875\n",
      "0 99 0.0729 0.1731752999096012 94.96527777777777\n",
      "0 100 0.0729 0.18047932043035822 94.921875\n",
      "val loss: 0.16184079901510526 val acc: 94.73157051282051\n",
      "0 101 0.0729 0.13127509545712557 96.09375\n",
      "0 102 0.0729 0.10988030974813764 96.484375\n",
      "0 103 0.0729 0.12331815155729564 96.09375\n",
      "0 104 0.0729 0.12350883332694332 96.2890625\n",
      "0 105 0.0729 0.1150113373522933 96.5625\n",
      "0 106 0.0729 0.12556300247948396 95.96354166666667\n",
      "0 107 0.0729 0.1240372556425763 96.09375\n",
      "0 108 0.0729 0.12780238018642112 96.09375\n",
      "0 109 0.0729 0.1338543396161013 96.00694444444444\n",
      "0 110 0.0729 0.13040385628440426 95.9375\n",
      "val loss: 0.1511466125409099 val acc: 95.3125\n",
      "0 111 0.0729 0.17231129639376952 96.09375\n",
      "0 112 0.0729 0.16167340386205414 96.09375\n",
      "0 113 0.0729 0.17074940654419776 96.09375\n",
      "0 114 0.0729 0.158162383592793 96.2890625\n",
      "0 115 0.0729 0.18569826550645768 95.625\n",
      "0 116 0.0729 0.17827049892960622 95.96354166666667\n",
      "0 117 0.0729 0.18752873194412228 95.20089285714286\n",
      "0 118 0.0729 0.20107813517182066 94.62890625\n",
      "0 119 0.06561 0.20110827090741443 94.53125\n",
      "0 120 0.06561 0.18758174747231962 94.921875\n",
      "val loss: 0.17395881950333406 val acc: 94.39102564102564\n",
      "0 121 0.06561 0.1497056524969764 94.53125\n",
      "0 122 0.06561 0.15579999925195775 95.3125\n",
      "0 123 0.06561 0.13950804757655874 95.57291666666667\n",
      "0 124 0.06561 0.1307776825776851 96.09375\n",
      "0 125 0.06561 0.14867347047168838 95.625\n",
      "0 126 0.06561 0.15294313714661137 94.921875\n",
      "0 127 0.06561 0.16748387966555645 94.86607142857143\n",
      "0 128 0.06561 0.16132085774688254 95.1171875\n",
      "0 129 0.06561 0.16337223106743518 95.39930555555556\n",
      "0 130 0.06561 0.1539079610853287 95.546875\n",
      "val loss: 0.11298152812256862 val acc: 96.46434294871794\n",
      "0 131 0.06561 0.11276388629407066 96.875\n",
      "0 132 0.06561 0.12157431153749917 96.484375\n",
      "0 133 0.06561 0.11694275510893164 96.875\n",
      "0 134 0.06561 0.11937044052730161 96.6796875\n",
      "0 135 0.06561 0.1123104874322508 97.03125\n",
      "0 136 0.06561 0.11377822895055768 96.875\n",
      "0 137 0.06561 0.12171395143096035 96.42857142857143\n",
      "0 138 0.06561 0.1214257476090704 96.484375\n",
      "0 139 0.06561 0.12198726600164081 96.52777777777777\n",
      "0 140 0.06561 0.119125009995837 96.5625\n",
      "val loss: 0.10786906996645043 val acc: 96.5244391025641\n",
      "0 141 0.06561 0.07968977699487197 96.09375\n",
      "0 142 0.06561 0.14471090145966606 94.53125\n",
      "0 143 0.06561 0.12611862956262787 95.57291666666667\n",
      "0 144 0.06561 0.12774416160280458 95.703125\n",
      "0 145 0.06561 0.12656584946166924 95.78125\n",
      "0 146 0.06561 0.12459306351299441 95.83333333333333\n",
      "0 147 0.06561 0.13483858946813346 95.64732142857143\n",
      "0 148 0.06561 0.13585785718247068 95.41015625\n",
      "0 149 0.05904900000000001 0.13376604330831873 95.74652777777777\n",
      "0 150 0.05904900000000001 0.1298667462998006 95.9375\n",
      "val loss: 0.10346537493541312 val acc: 96.70472756410257\n",
      "0 151 0.05904900000000001 0.2188753318138802 92.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 152 0.05904900000000001 0.16011923511877202 94.53125\n",
      "0 153 0.05904900000000001 0.184883916504562 94.27083333333333\n",
      "0 154 0.05904900000000001 0.1903653934338602 93.9453125\n",
      "0 155 0.05904900000000001 0.17301309620773936 95.0\n",
      "0 156 0.05904900000000001 0.16982731672499551 94.79166666666667\n",
      "0 157 0.05904900000000001 0.15888442071165385 95.08928571428571\n",
      "0 158 0.05904900000000001 0.14842805940596318 95.41015625\n",
      "0 159 0.05904900000000001 0.14125480509740057 95.74652777777777\n",
      "0 160 0.05904900000000001 0.13384257328442128 95.9375\n",
      "val loss: 0.09685911336237951 val acc: 96.77483974358974\n",
      "0 161 0.05904900000000001 0.10824250160076984 96.875\n",
      "0 162 0.05904900000000001 0.11791575600744401 96.484375\n",
      "0 163 0.05904900000000001 0.10997075566548054 96.875\n",
      "0 164 0.05904900000000001 0.09907966163389045 96.875\n",
      "0 165 0.05904900000000001 0.0951772351769137 97.1875\n",
      "0 166 0.05904900000000001 0.10484955824764804 96.74479166666667\n",
      "0 167 0.05904900000000001 0.09816771522594246 96.875\n",
      "0 168 0.05904900000000001 0.09604442681007369 97.0703125\n",
      "0 169 0.05904900000000001 0.09784546404475501 97.04861111111111\n",
      "0 170 0.05904900000000001 0.09816702872408142 96.953125\n",
      "val loss: 0.10171604407864852 val acc: 96.7948717948718\n",
      "0 171 0.05904900000000001 0.12207482805375505 96.09375\n",
      "0 172 0.05904900000000001 0.11057931212396231 96.875\n",
      "0 173 0.05904900000000001 0.09637428162146722 96.875\n",
      "0 174 0.05904900000000001 0.10277342485954474 96.09375\n",
      "0 175 0.05904900000000001 0.09311429255109754 96.5625\n",
      "0 176 0.05904900000000001 0.11143090809371536 96.484375\n",
      "0 177 0.05904900000000001 0.11436979589544831 96.42857142857143\n",
      "0 178 0.05904900000000001 0.11067361905879701 96.58203125\n",
      "0 179 0.05314410000000001 0.1187498132046318 96.44097222222223\n",
      "0 180 0.05314410000000001 0.12195530038343276 96.171875\n",
      "val loss: 0.10252867994415829 val acc: 96.82491987179488\n",
      "0 181 0.05314410000000001 0.17092351504521022 95.3125\n",
      "0 182 0.05314410000000001 0.11285412609406476 96.484375\n",
      "0 183 0.05314410000000001 0.1018519456523476 96.61458333333333\n",
      "0 184 0.05314410000000001 0.09730514681699662 96.875\n",
      "0 185 0.05314410000000001 0.08851035910311977 97.1875\n",
      "0 186 0.05314410000000001 0.09453728462180477 96.875\n",
      "0 187 0.05314410000000001 0.09552178217720518 96.875\n",
      "0 188 0.05314410000000001 0.10479046561742524 96.58203125\n",
      "0 189 0.05314410000000001 0.11081835406950297 96.61458333333333\n",
      "0 190 0.05314410000000001 0.10681185683537993 96.640625\n",
      "val loss: 0.09517069775719578 val acc: 96.93509615384616\n",
      "0 191 0.05314410000000001 0.15089116524434248 96.09375\n",
      "0 192 0.05314410000000001 0.1364307143489817 96.09375\n",
      "0 193 0.05314410000000001 0.12365822987944373 96.61458333333333\n",
      "0 194 0.05314410000000001 0.14656224301076254 96.09375\n",
      "0 195 0.05314410000000001 0.12899664658578788 96.25\n",
      "0 196 0.05314410000000001 0.12678623731611757 96.22395833333333\n",
      "0 197 0.05314410000000001 0.1272104881708977 96.09375\n",
      "0 198 0.05314410000000001 0.1230914250063188 96.2890625\n",
      "0 199 0.05314410000000001 0.11552865767052989 96.52777777777777\n",
      "0 200 0.05314410000000001 0.11597205615488011 96.40625\n",
      "val loss: 0.08487722315631947 val acc: 97.10536858974359\n",
      "0 201 0.05314410000000001 0.11714396133669333 95.3125\n",
      "0 202 0.05314410000000001 0.13907541069938933 94.921875\n",
      "0 203 0.05314410000000001 0.12385937006484582 96.09375\n",
      "0 204 0.05314410000000001 0.12966418056687806 95.3125\n",
      "0 205 0.05314410000000001 0.13642581231062212 95.15625\n",
      "0 206 0.05314410000000001 0.13919901460732806 95.05208333333333\n",
      "0 207 0.05314410000000001 0.1410030256282194 94.97767857142857\n",
      "0 208 0.05314410000000001 0.13712766229641762 95.01953125\n",
      "0 209 0.04782969000000001 0.13284512866500112 94.96527777777777\n",
      "0 210 0.04782969000000001 0.12835208569428302 95.15625\n",
      "val loss: 0.0817075703656928 val acc: 97.44591346153847\n",
      "0 211 0.04782969000000001 0.14667274807597425 95.3125\n",
      "0 212 0.04782969000000001 0.12601257500074764 95.703125\n",
      "0 213 0.04782969000000001 0.12270673709332895 96.35416666666667\n",
      "0 214 0.04782969000000001 0.1273641428561585 96.2890625\n",
      "0 215 0.04782969000000001 0.11994984377253273 96.71875\n",
      "0 216 0.04782969000000001 0.11525565796165697 96.875\n",
      "0 217 0.04782969000000001 0.12344593507371375 96.31696428571429\n",
      "0 218 0.04782969000000001 0.1136281381818149 96.58203125\n",
      "0 219 0.04782969000000001 0.11131782665473719 96.70138888888889\n",
      "0 220 0.04782969000000001 0.11088090522395706 96.640625\n",
      "val loss: 0.076821600392841 val acc: 97.60616987179488\n",
      "0 221 0.04782969000000001 0.12669890349750673 96.09375\n",
      "0 222 0.04782969000000001 0.10798348932795188 95.703125\n",
      "0 223 0.04782969000000001 0.09367039069843835 96.35416666666667\n",
      "0 224 0.04782969000000001 0.12071958168815786 95.5078125\n",
      "0 225 0.04782969000000001 0.104667105877904 96.09375\n",
      "0 226 0.04782969000000001 0.09578565695126691 96.484375\n",
      "0 227 0.04782969000000001 0.10489943085729385 96.20535714285714\n",
      "0 228 0.04782969000000001 0.1169683770395091 95.8984375\n",
      "0 229 0.04782969000000001 0.10986506445262248 96.18055555555556\n",
      "0 230 0.04782969000000001 0.11304852834040409 96.09375\n",
      "val loss: 0.07499035174969342 val acc: 97.55608974358974\n",
      "0 231 0.04782969000000001 0.08813725111964535 97.65625\n",
      "0 232 0.04782969000000001 0.0654400508342155 98.4375\n",
      "0 233 0.04782969000000001 0.08870365543774396 96.61458333333333\n",
      "0 234 0.04782969000000001 0.08578461976987603 96.875\n",
      "0 235 0.04782969000000001 0.07926177495129275 97.34375\n",
      "0 236 0.04782969000000001 0.09230755664418593 97.265625\n",
      "0 237 0.04782969000000001 0.11166211588199006 96.54017857142857\n",
      "0 238 0.04782969000000001 0.10246587366609956 96.875\n",
      "0 239 0.04304672100000001 0.09521775454360583 97.13541666666667\n",
      "0 240 0.04304672100000001 0.09321687900737907 97.109375\n",
      "val loss: 0.09454879766469873 val acc: 96.81490384615384\n",
      "0 241 0.04304672100000001 0.07236293829553389 96.875\n",
      "0 242 0.04304672100000001 0.08068081800215858 97.265625\n",
      "0 243 0.04304672100000001 0.09655232971529389 96.875\n",
      "0 244 0.04304672100000001 0.09403261447650176 97.265625\n",
      "0 245 0.04304672100000001 0.09849007699577739 97.03125\n",
      "0 246 0.04304672100000001 0.09013009935059335 97.265625\n",
      "0 247 0.04304672100000001 0.08690307678546837 97.32142857142857\n",
      "0 248 0.04304672100000001 0.09602806274997733 96.97265625\n",
      "0 249 0.04304672100000001 0.09828656496914569 96.70138888888889\n",
      "0 250 0.04304672100000001 0.09907857148036378 96.5625\n",
      "val loss: 0.07640501305226435 val acc: 97.5360576923077\n",
      "0 251 0.04304672100000001 0.05198772067933997 98.4375\n",
      "0 252 0.04304672100000001 0.05500849085575914 98.046875\n",
      "0 253 0.04304672100000001 0.06743342906447429 97.39583333333333\n",
      "0 254 0.04304672100000001 0.06336667480234547 98.046875\n",
      "0 255 0.04304672100000001 0.06866055141647108 97.65625\n",
      "0 256 0.04304672100000001 0.07501526315174445 97.65625\n",
      "0 257 0.04304672100000001 0.07695458094986904 97.65625\n",
      "0 258 0.04304672100000001 0.08592124087387792 97.55859375\n",
      "0 259 0.04304672100000001 0.08638715612002762 97.56944444444444\n",
      "0 260 0.04304672100000001 0.08481148660965784 97.578125\n",
      "val loss: 0.07287617895033736 val acc: 97.66626602564102\n",
      "0 261 0.04304672100000001 0.04707191810685729 99.21875\n",
      "0 262 0.04304672100000001 0.06162694092084375 98.046875\n",
      "0 263 0.04304672100000001 0.07515219358044699 98.17708333333333\n",
      "0 264 0.04304672100000001 0.0821839131561149 97.65625\n",
      "0 265 0.04304672100000001 0.08198413254627282 97.34375\n",
      "0 266 0.04304672100000001 0.08097981177146181 97.39583333333333\n",
      "0 267 0.04304672100000001 0.08995281668363335 97.20982142857143\n",
      "0 268 0.04304672100000001 0.08550121447629948 97.4609375\n",
      "0 269 0.03874204890000001 0.09256118981393784 97.30902777777777\n",
      "0 270 0.03874204890000001 0.09066933330657016 97.265625\n",
      "val loss: 0.08045278364127892 val acc: 97.51602564102564\n",
      "0 271 0.03874204890000001 0.07026099054087329 97.65625\n",
      "0 272 0.03874204890000001 0.05326196612548918 98.4375\n",
      "0 273 0.03874204890000001 0.08464659160394387 97.39583333333333\n",
      "0 274 0.03874204890000001 0.07988437587264698 97.65625\n",
      "0 275 0.03874204890000001 0.07670930808931742 97.34375\n",
      "0 276 0.03874204890000001 0.07784001825718223 97.13541666666667\n",
      "0 277 0.03874204890000001 0.08546550915760019 97.09821428571429\n",
      "0 278 0.03874204890000001 0.09118015152900208 96.97265625\n",
      "0 279 0.03874204890000001 0.09114803240268766 96.96180555555556\n",
      "0 280 0.03874204890000001 0.08615517062855618 97.1875\n",
      "val loss: 0.07439308335391857 val acc: 97.59615384615384\n",
      "0 281 0.03874204890000001 0.13007018608731682 97.65625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 282 0.03874204890000001 0.09804179675004193 97.65625\n",
      "0 283 0.03874204890000001 0.08807347990164026 97.65625\n",
      "0 284 0.03874204890000001 0.0819673631424076 97.65625\n",
      "0 285 0.03874204890000001 0.09527203238953341 97.34375\n",
      "0 286 0.03874204890000001 0.08664452755274525 97.52604166666667\n",
      "0 287 0.03874204890000001 0.08712574656820189 97.54464285714286\n",
      "0 288 0.03874204890000001 0.08934160739017233 97.65625\n",
      "0 289 0.03874204890000001 0.08820022528793255 97.65625\n",
      "0 290 0.03874204890000001 0.08768864179708936 97.578125\n",
      "val loss: 0.06764815329395468 val acc: 97.71634615384616\n",
      "0 291 0.03874204890000001 0.0698288674775282 99.21875\n",
      "0 292 0.03874204890000001 0.04470730426907987 99.21875\n",
      "0 293 0.03874204890000001 0.05990864109661844 98.69791666666667\n",
      "0 294 0.03874204890000001 0.05549427170189013 98.828125\n",
      "0 295 0.03874204890000001 0.05440410766323509 98.59375\n",
      "0 296 0.03874204890000001 0.07109807527269493 97.91666666666667\n",
      "0 297 0.03874204890000001 0.07874862689316767 97.54464285714286\n",
      "0 298 0.03874204890000001 0.07311246130986707 97.75390625\n",
      "0 299 0.03486784401000001 0.07260290386379435 97.74305555555556\n",
      "0 300 0.03486784401000001 0.08353219076457054 97.5\n",
      "val loss: 0.07123513090584266 val acc: 97.59615384615384\n",
      "0 301 0.03486784401000001 0.08998850564890909 98.4375\n",
      "0 302 0.03486784401000001 0.09427387060359273 98.828125\n",
      "0 303 0.03486784401000001 0.08810094251553986 98.17708333333333\n",
      "0 304 0.03486784401000001 0.07342739579917215 98.4375\n",
      "0 305 0.03486784401000001 0.06852835153660378 98.4375\n",
      "0 306 0.03486784401000001 0.06375886559283644 98.56770833333333\n",
      "0 307 0.03486784401000001 0.06458669579823136 98.54910714285714\n",
      "0 308 0.03486784401000001 0.07118011495572371 98.2421875\n",
      "0 309 0.03486784401000001 0.07622298331958956 98.00347222222223\n",
      "0 310 0.03486784401000001 0.07932948319455886 97.890625\n",
      "val loss: 0.07195474893339339 val acc: 97.64623397435898\n",
      "0 311 0.03486784401000001 0.020409291344338863 99.21875\n",
      "0 312 0.03486784401000001 0.04282222225271223 98.828125\n",
      "0 313 0.03486784401000001 0.04030199445430055 98.95833333333333\n",
      "0 314 0.03486784401000001 0.057943310819019725 98.4375\n",
      "0 315 0.03486784401000001 0.06564726443828198 98.125\n",
      "0 316 0.03486784401000001 0.06296604119170446 98.046875\n",
      "0 317 0.03486784401000001 0.06291995315337424 98.10267857142857\n",
      "0 318 0.03486784401000001 0.07261691833305965 97.94921875\n",
      "0 319 0.03486784401000001 0.07314580910332544 97.91666666666667\n",
      "0 320 0.03486784401000001 0.08001006714401558 97.734375\n",
      "val loss: 0.06890420236695893 val acc: 97.75641025641026\n",
      "0 321 0.03486784401000001 0.069552954772953 98.4375\n",
      "0 322 0.03486784401000001 0.10272396469927036 97.65625\n",
      "0 323 0.03486784401000001 0.09683230728541359 97.65625\n",
      "0 324 0.03486784401000001 0.09279335978667623 97.65625\n",
      "0 325 0.03486784401000001 0.09018987102184026 97.8125\n",
      "0 326 0.03486784401000001 0.08818397626080865 97.78645833333333\n",
      "0 327 0.03486784401000001 0.08309195225139125 97.99107142857143\n",
      "0 328 0.03486784401000001 0.08156594598026282 97.8515625\n",
      "0 329 0.031381059609000006 0.0868090345137322 97.48263888888889\n",
      "0 330 0.031381059609000006 0.08526594674880841 97.421875\n",
      "val loss: 0.06445853411629712 val acc: 97.76642628205128\n",
      "0 331 0.031381059609000006 0.0697575994192722 97.65625\n",
      "0 332 0.031381059609000006 0.09808778975249415 96.875\n",
      "0 333 0.031381059609000006 0.11384458912839841 96.35416666666667\n",
      "0 334 0.031381059609000006 0.10589921296414975 96.6796875\n",
      "0 335 0.031381059609000006 0.09750521439634832 96.71875\n",
      "0 336 0.031381059609000006 0.08541218646221226 97.13541666666667\n",
      "0 337 0.031381059609000006 0.0903923793882467 96.98660714285714\n",
      "0 338 0.031381059609000006 0.08902246068628972 96.875\n",
      "0 339 0.031381059609000006 0.09103683601712141 96.875\n",
      "0 340 0.031381059609000006 0.09749365594745776 96.796875\n",
      "val loss: 0.0656072690714021 val acc: 97.84655448717949\n",
      "0 341 0.031381059609000006 0.07602033346327698 96.875\n",
      "0 342 0.031381059609000006 0.13847358264484008 96.09375\n",
      "0 343 0.031381059609000006 0.10771464597777486 96.875\n",
      "0 344 0.031381059609000006 0.09744855753806873 97.0703125\n",
      "0 345 0.031381059609000006 0.09149658379423183 97.03125\n",
      "0 346 0.031381059609000006 0.09673797120352119 96.74479166666667\n",
      "0 347 0.031381059609000006 0.09356361632696379 96.76339285714286\n",
      "0 348 0.031381059609000006 0.09813305736684075 96.58203125\n",
      "0 349 0.031381059609000006 0.09505863503960299 96.70138888888889\n",
      "0 350 0.031381059609000006 0.09439160909470713 96.71875\n",
      "val loss: 0.07614012717293926 val acc: 97.47596153846153\n",
      "0 351 0.031381059609000006 0.17777617714627808 95.3125\n",
      "0 352 0.031381059609000006 0.15102637612385122 96.484375\n",
      "0 353 0.031381059609000006 0.12616505089963126 97.13541666666667\n",
      "0 354 0.031381059609000006 0.10376951578039352 97.65625\n",
      "0 355 0.031381059609000006 0.11396297496708425 97.65625\n",
      "0 356 0.031381059609000006 0.12112801107204901 97.265625\n",
      "0 357 0.031381059609000006 0.12303580580251013 96.98660714285714\n",
      "0 358 0.031381059609000006 0.11043428326440552 97.36328125\n",
      "0 359 0.028242953648100012 0.11244889348560844 97.22222222222223\n",
      "0 360 0.028242953648100012 0.10852397420559261 97.34375\n",
      "val loss: 0.07080238607033133 val acc: 97.6161858974359\n",
      "0 361 0.028242953648100012 0.12912946029088415 96.09375\n",
      "0 362 0.028242953648100012 0.08033654418526777 97.65625\n",
      "0 363 0.028242953648100012 0.0765754601572852 97.91666666666667\n",
      "0 364 0.028242953648100012 0.06726962732650235 97.8515625\n",
      "0 365 0.028242953648100012 0.06623811946232286 97.65625\n",
      "0 366 0.028242953648100012 0.06307196914058384 97.78645833333333\n",
      "0 367 0.028242953648100012 0.07399563807768704 97.54464285714286\n",
      "0 368 0.028242953648100012 0.07519939600351865 97.4609375\n",
      "0 369 0.028242953648100012 0.07112236521633417 97.65625\n",
      "0 370 0.028242953648100012 0.0739847319824829 97.578125\n",
      "val loss: 0.06599413552301964 val acc: 97.9266826923077\n",
      "0 371 0.028242953648100012 0.05568717454836677 97.65625\n",
      "0 372 0.028242953648100012 0.05267201913733803 97.65625\n",
      "0 373 0.028242953648100012 0.07785129437702813 96.875\n",
      "0 374 0.028242953648100012 0.08048468087880516 96.875\n",
      "0 375 0.028242953648100012 0.07762203399936196 97.1875\n",
      "0 376 0.028242953648100012 0.0744852428834439 97.39583333333333\n",
      "0 377 0.028242953648100012 0.07544927012214764 97.54464285714286\n",
      "0 378 0.028242953648100012 0.07349225492470281 97.65625\n",
      "0 379 0.028242953648100012 0.07243199285016456 97.65625\n",
      "0 380 0.028242953648100012 0.07448319430516921 97.734375\n",
      "val loss: 0.05898977467864739 val acc: 98.0068108974359\n",
      "0 381 0.028242953648100012 0.08387044690336712 96.875\n",
      "0 382 0.028242953648100012 0.0653467015246933 98.046875\n",
      "0 383 0.028242953648100012 0.08223998026937503 97.65625\n",
      "0 384 0.028242953648100012 0.07970269235516489 97.65625\n",
      "0 385 0.028242953648100012 0.0750449747400497 97.96875\n",
      "0 386 0.028242953648100012 0.07191437277532067 98.046875\n",
      "0 387 0.028242953648100012 0.06471299397114447 98.32589285714286\n",
      "0 388 0.028242953648100012 0.06303534133525812 98.33984375\n",
      "0 389 0.02541865828329001 0.06522904670355087 98.00347222222223\n",
      "0 390 0.02541865828329001 0.06962948810817168 98.046875\n",
      "val loss: 0.05970382190426557 val acc: 98.21714743589743\n",
      "0 391 0.02541865828329001 0.04276636272369648 98.4375\n",
      "0 392 0.02541865828329001 0.10304708885264313 96.484375\n",
      "0 393 0.02541865828329001 0.0970977115426888 96.61458333333333\n",
      "0 394 0.02541865828329001 0.08961152387700798 96.6796875\n",
      "0 395 0.02541865828329001 0.09465054633246277 96.71875\n",
      "0 396 0.02541865828329001 0.09712676791541987 96.875\n",
      "0 397 0.02541865828329001 0.10160595963663262 96.65178571428571\n",
      "0 398 0.02541865828329001 0.09874810632172548 96.77734375\n",
      "0 399 0.02541865828329001 0.09809145700712049 96.96180555555556\n",
      "0 400 0.02541865828329001 0.09358932204077673 97.1875\n",
      "val loss: 0.05854280773243278 val acc: 98.23717948717949\n",
      "0 401 0.02541865828329001 0.04350179495266354 99.21875\n",
      "0 402 0.02541865828329001 0.03600318587243668 99.609375\n",
      "0 403 0.02541865828329001 0.03671224825372491 99.73958333333333\n",
      "0 404 0.02541865828329001 0.04137640867722911 99.609375\n",
      "0 405 0.02541865828329001 0.045735639799928375 99.375\n",
      "0 406 0.02541865828329001 0.044401806522186726 99.34895833333333\n",
      "0 407 0.02541865828329001 0.05805694167422278 98.77232142857143\n",
      "0 408 0.02541865828329001 0.05360973696212923 98.92578125\n",
      "0 409 0.02541865828329001 0.05517732006760058 98.87152777777777\n",
      "0 410 0.02541865828329001 0.05153154650773563 98.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.058426848213681576 val acc: 98.15705128205128\n",
      "0 411 0.02541865828329001 0.10642414450768986 98.4375\n",
      "0 412 0.02541865828329001 0.0735460838152383 98.828125\n",
      "0 413 0.02541865828329001 0.06687960489632887 98.95833333333333\n",
      "0 414 0.02541865828329001 0.06194828577964619 99.0234375\n",
      "0 415 0.02541865828329001 0.06857503837310912 98.59375\n",
      "0 416 0.02541865828329001 0.0699946439579595 98.30729166666667\n",
      "0 417 0.02541865828329001 0.07150057352904908 98.32589285714286\n",
      "0 418 0.02541865828329001 0.07074863663548478 98.14453125\n",
      "0 419 0.02287679245496101 0.06915345506104577 98.09027777777777\n",
      "0 420 0.02287679245496101 0.0712404887787128 98.125\n",
      "val loss: 0.05548056907914125 val acc: 98.23717948717949\n",
      "0 421 0.02287679245496101 0.031108229971646487 100.0\n",
      "0 422 0.02287679245496101 0.029117237111116103 99.609375\n",
      "0 423 0.02287679245496101 0.043216096914059025 98.69791666666667\n",
      "0 424 0.02287679245496101 0.03907499082036929 98.828125\n",
      "0 425 0.02287679245496101 0.045460641881898384 98.59375\n",
      "0 426 0.02287679245496101 0.04584586226710178 98.56770833333333\n",
      "0 427 0.02287679245496101 0.0498637239404433 98.4375\n",
      "0 428 0.02287679245496101 0.05078657716331985 98.4375\n",
      "0 429 0.02287679245496101 0.0625465312202151 98.26388888888889\n",
      "0 430 0.02287679245496101 0.06027491923169372 98.28125\n",
      "val loss: 0.0540905725086459 val acc: 98.3173076923077\n",
      "0 431 0.02287679245496101 0.059916510285987074 98.4375\n",
      "0 432 0.02287679245496101 0.07998525531107356 96.875\n",
      "0 433 0.02287679245496101 0.08349772636701779 96.61458333333333\n",
      "0 434 0.02287679245496101 0.07566734002122463 97.0703125\n",
      "0 435 0.02287679245496101 0.08062318644257722 97.1875\n",
      "0 436 0.02287679245496101 0.08015384883367443 97.39583333333333\n",
      "0 437 0.02287679245496101 0.07917909738847949 97.32142857142857\n",
      "0 438 0.02287679245496101 0.0829211282191982 97.265625\n",
      "0 439 0.02287679245496101 0.0886097485271047 97.04861111111111\n",
      "0 440 0.02287679245496101 0.08887000699303281 97.109375\n",
      "val loss: 0.0537569905518086 val acc: 98.19711538461539\n",
      "0 441 0.02287679245496101 0.07923161562494853 99.21875\n",
      "0 442 0.02287679245496101 0.07950732196523108 98.046875\n",
      "0 443 0.02287679245496101 0.08453321051402972 97.39583333333333\n",
      "0 444 0.02287679245496101 0.0846818578339129 97.65625\n",
      "0 445 0.02287679245496101 0.07333550767623491 98.125\n",
      "0 446 0.02287679245496101 0.0724597767516929 97.91666666666667\n",
      "0 447 0.02287679245496101 0.07725687388475601 97.87946428571429\n",
      "0 448 0.02287679245496101 0.07882761080333796 97.65625\n",
      "0 449 0.02058911320946491 0.07556967891301196 97.74305555555556\n",
      "0 450 0.02058911320946491 0.0737901952791851 97.8125\n",
      "val loss: 0.05657668020242525 val acc: 98.11698717948718\n",
      "0 451 0.02058911320946491 0.04937932703947871 98.4375\n",
      "0 452 0.02058911320946491 0.06575606215564236 98.046875\n",
      "0 453 0.02058911320946491 0.06633734100625095 97.91666666666667\n",
      "0 454 0.02058911320946491 0.07160612361659104 97.65625\n",
      "0 455 0.02058911320946491 0.07463676624851986 97.65625\n",
      "0 456 0.02058911320946491 0.07317454206704847 97.65625\n",
      "0 457 0.02058911320946491 0.07028698616917291 97.76785714285714\n",
      "0 458 0.02058911320946491 0.06381871498115592 98.046875\n",
      "0 459 0.02058911320946491 0.06569525828295769 97.91666666666667\n",
      "0 460 0.02058911320946491 0.0617663609519727 98.046875\n",
      "val loss: 0.05540437252787911 val acc: 98.14703525641026\n",
      "0 461 0.02058911320946491 0.05029692924872197 99.21875\n",
      "0 462 0.02058911320946491 0.04821573489567885 99.21875\n",
      "0 463 0.02058911320946491 0.07932285404883614 97.91666666666667\n",
      "0 464 0.02058911320946491 0.07217740912616653 98.2421875\n",
      "0 465 0.02058911320946491 0.0630191359272011 98.4375\n",
      "0 466 0.02058911320946491 0.05814170829941806 98.56770833333333\n",
      "0 467 0.02058911320946491 0.05771655488197551 98.66071428571429\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    index = [i for i in range(images.shape[0])]\n",
    "    shuffle(index)\n",
    "    images = images[index]\n",
    "    labels = labels[index]\n",
    "    # train\n",
    "    for i in range(images.shape[0] // batch_size):\n",
    "        step+=1\n",
    "        # learning rate decay\n",
    "        lr = init_lr*0.9**(step//30)\n",
    "        # forward\n",
    "        img = images[i * batch_size:(i + 1) * batch_size].reshape([batch_size, 28, 28, 1]).transpose(0,3,1,2)\n",
    "        img = np.pad(img, [[0,0],[0,0],[2,2],[2,2]], mode='constant')/255.0\n",
    "        img = (img-0.1307)/0.3081\n",
    "        label = labels[i * batch_size:(i + 1) * batch_size]\n",
    "        out, conv1_cache = conv1.forward(img)\n",
    "        out, bn1_cache = bn1.forward(out)\n",
    "        out, relu1_cache = relu1.forward(out)\n",
    "        out, pool1_cache = pool1.forward(out)\n",
    "\n",
    "        out, conv2_cache = conv2.forward(out)\n",
    "        out, bn2_cache = bn2.forward(out)\n",
    "        out, relu2_cache = relu2.forward(out)\n",
    "        out, pool2_cache = pool2.forward(out)\n",
    "\n",
    "        out, conv3_cache = conv3.forward(out)\n",
    "        out, bn3_cache = bn3.forward(out)\n",
    "        out, relu3_cache = relu3.forward(out)\n",
    "\n",
    "        conv_out = out\n",
    "\n",
    "        out = conv_out.reshape(batch_size, -1)\n",
    "        out, fc1_cache = fc1.forward(out)\n",
    "        out, relu4_cache = relu4.forward(out)\n",
    "        out, dp_cache = dp.forward(out)\n",
    "\n",
    "        out, fc2_cache = fc2.forward(out)\n",
    "        loss, dx =  sf.forward_and_backward(out, np.array(label))\n",
    "\n",
    "        # calculate gradient\n",
    "        _, _, dx = fc2.gradient(dx, fc2_cache)\n",
    "\n",
    "        dx = dp.gradient(dx, dp_cache)\n",
    "        dx = relu4.gradient(dx, relu4_cache)\n",
    "        _,_,dx = fc1.gradient(dx, fc1_cache)\n",
    "\n",
    "        dx = dx.reshape(conv_out.shape)\n",
    "\n",
    "        dx = relu3.gradient(dx, relu3_cache)\n",
    "        _,_,dx = bn3.gradient(dx, bn3_cache)\n",
    "        _,_,dx = conv3.gradient(dx, conv3_cache)\n",
    "\n",
    "\n",
    "        dx = pool2.gradient(dx, pool2_cache)\n",
    "        dx = relu2.gradient(dx, relu2_cache)\n",
    "        _, _, dx = bn2.gradient(dx, bn2_cache)\n",
    "        _, _ ,dx = conv2.gradient(dx, conv2_cache)\n",
    "\n",
    "        dx = pool1.gradient(dx, pool1_cache)\n",
    "        dx = relu1.gradient(dx, relu1_cache)\n",
    "        _, _, dx = bn1.gradient(dx, bn1_cache)\n",
    "        _,_, dx = conv1.gradient(dx, conv1_cache)\n",
    "\n",
    "        # backward\n",
    "        conv1.backward(lr)\n",
    "        bn1.backward(lr)\n",
    "        conv2.backward(lr)\n",
    "        bn2.backward(lr)\n",
    "        conv3.backward(lr)\n",
    "        bn3.backward(lr)\n",
    "        fc1.backward(lr)\n",
    "        fc2.backward(lr)\n",
    "        \n",
    "        \n",
    "        pred = np.argmax(out, axis=1)\n",
    "        correct = pred.__eq__(label).sum()\n",
    "        train_acc.update(correct/label.size*100)\n",
    "        train_loss.update(loss)\n",
    "\n",
    "\n",
    "        print(epoch,i,lr, train_loss.avg,train_acc.avg)\n",
    "\n",
    "        if i%10==0:\n",
    "            for k in range(test_images.shape[0] // batch_size):\n",
    "                batch_acc = 0\n",
    "                img = test_images[k * batch_size:(k + 1) * batch_size].reshape([batch_size, 28, 28, 1]).transpose(0, 3, 1, 2)\n",
    "                img = np.pad(img, [[0, 0], [0, 0], [2, 2], [2, 2]], mode='constant')/255.0\n",
    "                img = (img-0.1307)/0.3081\n",
    "                label = test_labels[k * batch_size:(k + 1) * batch_size]\n",
    "\n",
    "                out, conv1_cache = conv1.forward(img)\n",
    "                out, bn1_cache = bn1.forward(out, False)\n",
    "                out, relu1_cache = relu1.forward(out)\n",
    "                out, pool1_cache = pool1.forward(out)\n",
    "\n",
    "                out, conv2_cache = conv2.forward(out)\n",
    "                out, bn2_cache = bn2.forward(out, False)\n",
    "                out, relu2_cache = relu2.forward(out)\n",
    "                out, pool2_cache = pool2.forward(out)\n",
    "                out, conv3_cache = conv3.forward(out)\n",
    "                out, bn3_cache = bn3.forward(out, False)\n",
    "                out, relu3_cache = relu3.forward(out)\n",
    "\n",
    "                conv_out = out\n",
    "\n",
    "                out = conv_out.reshape(batch_size, -1)\n",
    "                out, fc1_cache = fc1.forward(out)\n",
    "                out, relu4_cache = relu4.forward(out)\n",
    "                out = dp.forward(out, False)\n",
    "\n",
    "                out, fc2_cache = fc2.forward(out)\n",
    "                loss, dx =  sf.forward_and_backward(out, np.array(label))\n",
    "                \n",
    "                pred = np.argmax(out, axis=1)\n",
    "                correct = pred.__eq__(label).sum()\n",
    "                val_acc.update(correct/label.size*100)\n",
    "                val_loss.update(loss)\n",
    "            print(\"val loss:\", val_loss.avg, \"val acc:\", val_acc.avg)\n",
    "            \n",
    "            train_acc_all.append(train_acc.avg)\n",
    "            train_loss_all.append(train_loss.avg)\n",
    "            val_acc_all.append(val_acc.avg)\n",
    "            val_loss_all.append(val_loss.avg)\n",
    "            \n",
    "            val_loss.reset()\n",
    "            val_acc.reset()\n",
    "            train_acc.reset()\n",
    "            train_loss.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN shows a training accuracy of 98.66% and validation accuracy of 98.15% after just one epoch of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
