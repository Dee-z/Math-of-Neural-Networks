{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short Term Memory Neural Network\n",
    "\n",
    "We're going to build a Long Short Term Memory Neural Network from scratch and look at the math behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mvp\\AppData\\Roaming\\Python\\Python37\\site-packages\\requests\\__init__.py:91: RequestsDependencyWarning: urllib3 (1.25.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import tensorflow as tf\n",
    "plt.style.use('seaborn-white')\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Dataset\n",
    "\n",
    "We will be using the Shakespeare dataset to train an LSTM to generate similar texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "data = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "Once we have the dataset we need to one-hot encode by character. To do that later, we will create a dictionary for character to index and vice-versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(data))\n",
    "data_size, X_size = len(data), len(chars)\n",
    "print(\"data has %d characters, %d unique\" % (data_size, X_size))\n",
    "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we have our one-hot encoding dictionary in place, we can move on to the LSTM!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the Long Short-Term Memory (LSTM) Cell\n",
    "\n",
    "A vanilla RNN suffers from [the vanishing gradients problem](http://neuralnetworksanddeeplearning.com/chap5.html#the_vanishing_gradient_problem) which gives challenges in saving memory over longer sequences. To combat these issues the gated hidden units were created. The two most prominent gated hidden units are the Long Short-Term Memory (LSTM) cell and the Gated Recurrent Unit (GRU), both of which have shown increased performance in saving and reusing memory in later timesteps. In this exercise, we will focus on LSTM but you would easily be able to go ahead and implement the GRU as well based on the principles that you learn here.\n",
    "\n",
    "Below is a figure of the LSTM cell:\n",
    "\n",
    "![lstm](https://i.imgur.com/3VkmUCe.png)\n",
    "Source: https://arxiv.org/abs/1412.7828\n",
    "\n",
    "\n",
    "The LSTM cell contains three gates, input, forget, output gates and a memory cell.\n",
    "The output of the LSTM unit is computed with the following functions, where $\\sigma = \\mathrm{softmax}$.\n",
    "We have input gate $i$, forget gate $f$, and output gate $o$ defines as\n",
    "\n",
    "- $i = \\sigma ( W^i [h_{t-1}, x_t])$\n",
    "\n",
    "- $f = \\sigma ( W^f [h_{t-1},x_t])$\n",
    "\n",
    "- $o = \\sigma ( W^o [h_{t-1},x_t])$\n",
    "\n",
    "where $W^i, W^f, W^o$ are weight matrices applied to a concatenated $h_{t-1}$ (hidden state vector) and $x_t$ (input vector)  for each respective gate.\n",
    "\n",
    "$h_{t-1}$, from the previous time step along with the current input $x_t$ are used to compute the a candidate $g$\n",
    "\n",
    "- $g = \\mathrm{tanh}( W^g [h_{t-1}, x_t])$\n",
    "\n",
    "The value of the cell's memory, $c_t$, is updated as\n",
    "\n",
    "- $c_t = c_{t-1} \\circ f + g \\circ i$\n",
    "\n",
    "where $c_{t-1}$ is the previous memory, and $\\circ$ refers to element-wise multiplication.\n",
    "\n",
    "The output, $h_t$, is computed as\n",
    "\n",
    "- $h_t = \\mathrm{tanh}(c_t) \\circ o$\n",
    "\n",
    "and it is used for both the timestep's output and the next timestep, whereas $c_t$ is exclusively sent to the next timestep.\n",
    "This makes $c_t$ a memory feature, and is not used directly to compute the output of the timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions and Derivatives\n",
    "\n",
    "Let us define activation functions and its derivative for future use \n",
    "\n",
    "#### Sigmoid\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma(x) &= \\frac{1}{1 + e^{-x}}\\\\\n",
    "\\frac{d\\sigma(x)}{dx} &= \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "\\end{align}\n",
    "\n",
    "#### Tanh\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d\\text{tanh}(x)}{dx} &= 1 - \\text{tanh}^2(x)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters and Parameters\n",
    "\n",
    "Let us define the hyperparameters fo the network along with a parameter class to declare the weights of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_size = 100 # Size of the hidden layer\n",
    "T_steps = 25 # Number of time steps (length of the sequence) used for training\n",
    "learning_rate = 1e-1 # Learning rate\n",
    "weight_sd = 0.1 # Standard deviation of weights for initialization\n",
    "z_size = H_size + X_size # Size of concatenate(H, X) vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Param will have a value, its derivative and its momentum (for use in Adagrad later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Param:\n",
    "    def __init__(self, name, value):\n",
    "        self.name = name\n",
    "        self.v = value #parameter value\n",
    "        self.d = np.zeros_like(value) #derivative\n",
    "        self.m = np.zeros_like(value) #momentum for AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use random weights with normal distribution (`0`, `weight_sd`) for $tanh$ activation function and (`0.5`, `weight_sd`) for $sigmoid$ activation function.\n",
    "\n",
    "Biases are initialized to zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def __init__(self):\n",
    "        self.W_f = Param('W_f', \n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_f = Param('b_f',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_i = Param('W_i',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_i = Param('b_i',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_C = Param('W_C',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd)\n",
    "        self.b_C = Param('b_C',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_o = Param('W_o',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_o = Param('b_o',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        #For final layer to predict the next character\n",
    "        self.W_v = Param('W_v',\n",
    "                         np.random.randn(X_size, H_size) * weight_sd)\n",
    "        self.b_v = Param('b_v',\n",
    "                         np.zeros((X_size, 1)))\n",
    "        \n",
    "    def all(self):\n",
    "        return [self.W_f, self.W_i, self.W_C, self.W_o, self.W_v,\n",
    "               self.b_f, self.b_i, self.b_C, self.b_o, self.b_v]\n",
    "        \n",
    "parameters = Parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialiation of a LSTM network\n",
    "\n",
    "We will implement the forward pass, backward pass, optimization and training loop, now for an LSTM in numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass\n",
    "\n",
    "![LSTM](http://blog.varunajayasiri.com/ml/lstm.svg)\n",
    "\n",
    "*Operation $z$ is the concatenation of $x$ and $h_{t-1}$*\n",
    "\n",
    "### Concatenation of $h_{t-1}$ and $x_t$\n",
    "\\begin{align}\n",
    "z & = [h_{t-1}, x_t] \\\\\n",
    "\\end{align}\n",
    "\n",
    "### LSTM functions\n",
    "\\begin{align}\n",
    "f_t & = \\sigma(W_f \\cdot z + b_f) \\\\\n",
    "i_t & = \\sigma(W_i \\cdot z + b_i) \\\\\n",
    "\\bar{C}_t & = tanh(W_C \\cdot z + b_C) \\\\\n",
    "C_t & = f_t * C_{t-1} + i_t * \\bar{C}_t \\\\\n",
    "o_t & = \\sigma(W_o \\cdot z + b_t) \\\\\n",
    "h_t &= o_t * tanh(C_t) \\\\\n",
    "\\end{align}\n",
    "\n",
    "### Logits\n",
    "\\begin{align}\n",
    "v_t &= W_v \\cdot h_t + b_v \\\\\n",
    "\\end{align}\n",
    "\n",
    "### Softmax\n",
    "\\begin{align}\n",
    "\\hat{y_t} &= \\text{softmax}(v_t)\n",
    "\\end{align}\n",
    "\n",
    "$\\hat{y_t}$ is `y` in code and $y_t$ is `targets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, h_prev, C_prev, p = parameters):\n",
    "    assert x.shape == (X_size, 1)\n",
    "    assert h_prev.shape == (H_size, 1)\n",
    "    assert C_prev.shape == (H_size, 1)\n",
    "    \n",
    "    z = np.row_stack((h_prev, x))\n",
    "    f = sigmoid(np.dot(p.W_f.v, z) + p.b_f.v)\n",
    "    i = sigmoid(np.dot(p.W_i.v, z) + p.b_i.v)\n",
    "    C_bar = tanh(np.dot(p.W_C.v, z) + p.b_C.v)\n",
    "\n",
    "    C = f * C_prev + i * C_bar\n",
    "    o = sigmoid(np.dot(p.W_o.v, z) + p.b_o.v)\n",
    "    h = o * tanh(C)\n",
    "\n",
    "    v = np.dot(p.W_v.v, h) + p.b_v.v\n",
    "    y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
    "\n",
    "    return z, f, i, C_bar, C, o, h, v, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass\n",
    "\n",
    "### Loss\n",
    "\n",
    "\\begin{align}\n",
    "L_k &= -\\sum_{t=k}^T\\sum_j y_{t,j} log \\hat{y_{t,j}} \\\\\n",
    "L &= L_1 \\\\\n",
    "\\end{align}\n",
    "\n",
    "### Gradients\n",
    "\n",
    "\\begin{align}\n",
    "dv_t &= \\hat{y_t} - y_t \\\\\n",
    "dh_t &= dh'_t + W_y^T \\cdot dv_t \\\\\n",
    "do_t &= dh_t * \\text{tanh}(C_t) \\\\\n",
    "dC_t &= dC'_t + dh_t * o_t * (1 - \\text{tanh}^2(C_t))\\\\\n",
    "d\\bar{C}_t &= dC_t * i_t \\\\\n",
    "di_t &= dC_t * \\bar{C}_t \\\\\n",
    "df_t &= dC_t * C_{t-1} \\\\\n",
    "\\\\\n",
    "df'_t &= f_t * (1 - f_t) * df_t \\\\\n",
    "di'_t &= i_t * (1 - i_t) * di_t \\\\\n",
    "d\\bar{C}'_{t-1} &= (1 - \\bar{C}_t^2) * d\\bar{C}_t \\\\\n",
    "do'_t &= o_t * (1 - o_t) * do_t \\\\\n",
    "dz_t &= W_f^T \\cdot df'_t \\\\\n",
    "     &+ W_i^T \\cdot di_t \\\\\n",
    "     &+ W_C^T \\cdot d\\bar{C}_t \\\\\n",
    "     &+ W_o^T \\cdot do_t \\\\\n",
    "\\\\\n",
    "[dh'_{t-1}, dx_t] &= dz_t \\\\\n",
    "dC'_t &= f_t * dC_t\n",
    "\\end{align}\n",
    "\n",
    "* $dC'_t = \\frac{\\partial L_{t+1}}{\\partial C_t}$ and $dh'_t = \\frac{\\partial L_{t+1}}{\\partial h_t}$\n",
    "* $dC_t = \\frac{\\partial L}{\\partial C_t} = \\frac{\\partial L_t}{\\partial C_t}$ and $dh_t = \\frac{\\partial L}{\\partial h_t} = \\frac{\\partial L_{t}}{\\partial h_t}$\n",
    "* All other derivatives are of $L$\n",
    "* `target` is target character index $y_t$\n",
    "* `dh_next` is $dh'_{t}$ (size H x 1)\n",
    "* `dC_next` is $dC'_{t}$ (size H x 1)\n",
    "* `C_prev` is $C_{t-1}$ (size H x 1)\n",
    "* $df'_t$, $di'_t$, $d\\bar{C}'_t$, and $do'_t$ are *also* assigned to `df`, `di`, `dC_bar`, and `do` in the **code**.\n",
    "* *Returns* $dh_t$ and $dC_t$\n",
    "\n",
    "### Model parameter gradients\n",
    "\n",
    "\\begin{align}\n",
    "dW_v &= dv_t \\cdot h_t^T \\\\\n",
    "db_v &= dv_t \\\\\n",
    "\\\\\n",
    "dW_f &= df'_t \\cdot z^T \\\\\n",
    "db_f &= df'_t \\\\\n",
    "\\\\\n",
    "dW_i &= di'_t \\cdot z^T \\\\\n",
    "db_i &= di'_t \\\\\n",
    "\\\\\n",
    "dW_C &= d\\bar{C}'_t \\cdot z^T \\\\\n",
    "db_C &= d\\bar{C}'_t \\\\\n",
    "\\\\\n",
    "dW_o &= do'_t \\cdot z^T \\\\\n",
    "db_o &= do'_t \\\\\n",
    "\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gradients(params = parameters):\n",
    "    for p in params.all():\n",
    "        p.d.fill(0)\n",
    "        \n",
    "def clip_gradients(params = parameters):\n",
    "    for p in params.all():\n",
    "        np.clip(p.d, -1, 1, out=p.d)\n",
    "\n",
    "def backward(target, dh_next, dC_next, C_prev,\n",
    "             z, f, i, C_bar, C, o, h, v, y,\n",
    "             p = parameters):\n",
    "    \n",
    "    assert z.shape == (X_size + H_size, 1)\n",
    "    assert v.shape == (X_size, 1)\n",
    "    assert y.shape == (X_size, 1)\n",
    "    \n",
    "    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
    "        assert param.shape == (H_size, 1)\n",
    "        \n",
    "    dv = np.copy(y)\n",
    "    dv[target] -= 1\n",
    "\n",
    "    p.W_v.d += np.dot(dv, h.T)\n",
    "    p.b_v.d += dv\n",
    "\n",
    "    dh = np.dot(p.W_v.v.T, dv)        \n",
    "    dh += dh_next\n",
    "    do = dh * tanh(C)\n",
    "    do = dsigmoid(o) * do\n",
    "    p.W_o.d += np.dot(do, z.T)\n",
    "    p.b_o.d += do\n",
    "\n",
    "    dC = np.copy(dC_next)\n",
    "    dC += dh * o * dtanh(tanh(C))\n",
    "    dC_bar = dC * i\n",
    "    dC_bar = dtanh(C_bar) * dC_bar\n",
    "    p.W_C.d += np.dot(dC_bar, z.T)\n",
    "    p.b_C.d += dC_bar\n",
    "\n",
    "    di = dC * C_bar\n",
    "    di = dsigmoid(i) * di\n",
    "    p.W_i.d += np.dot(di, z.T)\n",
    "    p.b_i.d += di\n",
    "\n",
    "    df = dC * C_prev\n",
    "    df = dsigmoid(f) * df\n",
    "    p.W_f.d += np.dot(df, z.T)\n",
    "    p.b_f.d += df\n",
    "\n",
    "    dz = (np.dot(p.W_f.v.T, df)\n",
    "         + np.dot(p.W_i.v.T, di)\n",
    "         + np.dot(p.W_C.v.T, dC_bar)\n",
    "         + np.dot(p.W_o.v.T, do))\n",
    "    dh_prev = dz[:H_size, :]\n",
    "    dC_prev = f * dC\n",
    "    \n",
    "    return dh_prev, dC_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and store the values in forward pass. Accumulate gradients in backward pass and clip gradients to avoid exploding gradients.\n",
    "\n",
    "* `input`, `target` are list of integers, with character indexes.\n",
    "* `h_prev` is the array of initial `h` at $h_{-1}$ (size H x 1)\n",
    "* `C_prev` is the array of initial `C` at $C_{-1}$ (size H x 1)\n",
    "* *Returns* loss, final $h_T$ and $C_T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward(inputs, targets, h_prev, C_prev):\n",
    "    global paramters\n",
    "    \n",
    "    # To store the values for each time step\n",
    "    x_s, z_s, f_s, i_s,  = {}, {}, {}, {}\n",
    "    C_bar_s, C_s, o_s, h_s = {}, {}, {}, {}\n",
    "    v_s, y_s =  {}, {}\n",
    "    \n",
    "    # Values at t - 1\n",
    "    h_s[-1] = np.copy(h_prev)\n",
    "    C_s[-1] = np.copy(C_prev)\n",
    "    \n",
    "    loss = 0\n",
    "    # Loop through time steps\n",
    "    assert len(inputs) == T_steps\n",
    "    for t in range(len(inputs)):\n",
    "        x_s[t] = np.zeros((X_size, 1))\n",
    "        x_s[t][inputs[t]] = 1 # Input character\n",
    "        \n",
    "        (z_s[t], f_s[t], i_s[t],\n",
    "        C_bar_s[t], C_s[t], o_s[t], h_s[t],\n",
    "        v_s[t], y_s[t]) = \\\n",
    "            forward(x_s[t], h_s[t - 1], C_s[t - 1]) # Forward pass\n",
    "            \n",
    "        loss += -np.log(y_s[t][targets[t], 0]) # Loss for at t\n",
    "        \n",
    "    clear_gradients()\n",
    "\n",
    "    dh_next = np.zeros_like(h_s[0]) #dh from the next character\n",
    "    dC_next = np.zeros_like(C_s[0]) #dh from the next character\n",
    "\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # Backward pass\n",
    "        dh_next, dC_next = \\\n",
    "            backward(target = targets[t], dh_next = dh_next,\n",
    "                     dC_next = dC_next, C_prev = C_s[t-1],\n",
    "                     z = z_s[t], f = f_s[t], i = i_s[t], C_bar = C_bar_s[t],\n",
    "                     C = C_s[t], o = o_s[t], h = h_s[t], v = v_s[t],\n",
    "                     y = y_s[t])\n",
    "\n",
    "    clip_gradients()\n",
    "        \n",
    "    return loss, h_s[len(inputs) - 1], C_s[len(inputs) - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the next character\n",
    "\n",
    "Lets also define a function to sample the next character for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h_prev, C_prev, first_char_idx, sentence_length):\n",
    "    x = np.zeros((X_size, 1))\n",
    "    x[first_char_idx] = 1\n",
    "\n",
    "    h = h_prev\n",
    "    C = C_prev\n",
    "\n",
    "    indexes = []\n",
    "    \n",
    "    for t in range(sentence_length):\n",
    "        _, _, _, _, C, _, h, _, p = forward(x, h, C)\n",
    "        idx = np.random.choice(range(X_size), p=p.ravel())\n",
    "        x = np.zeros((X_size, 1))\n",
    "        x[idx] = 1\n",
    "        indexes.append(idx)\n",
    "\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization (Adagrad)\n",
    "\n",
    "Update the graph and display a sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_status(inputs, h_prev, C_prev):\n",
    "    #initialized later\n",
    "    global plot_iter, plot_loss\n",
    "    global smooth_loss\n",
    "    \n",
    "    # Get predictions for 200 letters with current model\n",
    "\n",
    "    sample_idx = sample(h_prev, C_prev, inputs[0], 200)\n",
    "    txt = ''.join(idx_to_char[idx] for idx in sample_idx)\n",
    "\n",
    "    # Clear and plot\n",
    "    plt.plot(plot_iter, plot_loss)\n",
    "    display.clear_output(wait=True)\n",
    "    plt.show()\n",
    "\n",
    "    #Print prediction and loss\n",
    "    print(\"----\\n %s \\n----\" % (txt, ))\n",
    "    print(\"iter %d, loss %f\" % (iteration, smooth_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update parameters\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_i &= \\theta_i - \\eta\\frac{d\\theta_i}{\\sum dw_{\\tau}^2} \\\\\n",
    "d\\theta_i &= \\frac{\\partial L}{\\partial \\theta_i}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_paramters(params = parameters):\n",
    "    for p in params.all():\n",
    "        p.m += p.d * p.d # Calculate sum of gradients\n",
    "        #print(learning_rate * dparam)\n",
    "        p.v += -(learning_rate * p.d / np.sqrt(p.m + 1e-8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "We need to define a full training loop with a forward pass, backward pass, optimization step and validation. \n",
    "\n",
    "Before we do that let us define a class to ensure the training proccess isn't stopped in between epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "\n",
    "class DelayedKeyboardInterrupt(object):\n",
    "    def __enter__(self):\n",
    "        self.signal_received = False\n",
    "        self.old_handler = signal.signal(signal.SIGINT, self.handler)\n",
    "\n",
    "    def handler(self, sig, frame):\n",
    "        self.signal_received = (sig, frame)\n",
    "        print('SIGINT received. Delaying KeyboardInterrupt.')\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        signal.signal(signal.SIGINT, self.old_handler)\n",
    "        if self.signal_received:\n",
    "            self.old_handler(*self.signal_received)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also store the loss to graph it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential average of loss\n",
    "# Initialize to a error of a random model\n",
    "smooth_loss = -np.log(1.0 / X_size) * T_steps\n",
    "\n",
    "iteration, pointer = 0, 0\n",
    "\n",
    "# For the graph\n",
    "plot_iter = np.zeros((0))\n",
    "plot_loss = np.zeros((0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD0CAYAAACb6FGmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deZgU5dX38e8Mm6CAS3BJZIkLR40rLvgqCIqGICaaGKNRTNTHGPOg0eijGINC1LhFMe4awTWSqCiagCiKCMMiIAJCgJtdGGQblplhGWar94+uGRumq3qmp6tnGn+f65rrmu5T3X26qrpO1V1V953jeR4iIiK5DZ2AiIg0DioIIiICqCCIiIhPBUFERAAVBBER8TXN1AeZWQvgVGANUJGpzxURyXJNgEOAGc65nVF+UMYKArFikJfBzxMR2ZN0ByZF+QGZLAhrAF5//XUOPvjgDH6siEj2Wrt2LVdccQX429AoZbIgVAAcfPDBHHrooRn8WBGRPULkTe06qSwiIoAKgoiI+FQQREQEUEEQERGfCoKIiAC1vMrIzA4EZgLnAeXAy4AHzAP6O+cqzWwQ0NeP3+ycmx5JxiIiEomkRwhm1gx4HtjhPzUEGOic6w7kABeaWRegB9AVuAx4Op1Jfr1lB53uGE3e4g3pfFsREYlTmyajR4DngK/9xycDE/z/xwDnAt2Asc45zzm3EmhqZu3SleTMrzYD8K8Zq9L1liIispvQgmBmVwEbnHMfxj2d45yrGmatGGgLtAEK46apel5ERLJEsnMI1wCemZ0LnAi8ChwYF28NbAGK/P93f15ERLJE6BGCc+4s51wP51xPYDbwK2CMmfX0J+lDrMO6yUBvM8s1sw5ArnOuILq0RUQk3VLpy+hW4AUzaw4sAEY45yrMLA+YSqzI9E9jjiIikgG1Lgj+UUKVHgnig4HB9c5IREQahG5MExERQAVBRER8KggiIgKoIIiIiE8FQUREABUEERHxZVdB8JJPIiIiqcmKgpCT09AZiIjs+bKiIHg6MhARiVxWFIRqOlIQEYlMdhUEERGJjAqCiIgAKggiIuJTQRAREUAFQUREfCoIIiICqCCIiIgv6YhpZtYEeAEwoAK4GmgL/AdY7E/2rHPuDTMbBPQFyoGbnXPTI8laRETSrjZDaP4YwDl3ppn1BIYQKwZDnHOPVk1kZl2IDa3ZFWgPvA2cmu6ERUQkGkmbjJxz7wLX+Q87AuuAk4G+ZjbRzIaZWWugGzDWOec551YCTc2sXVqzVRcWIiKRqdU5BOdcuZm9AjwJjACmA7c5584ClgGDgDZAYdzLiok1LdWbOrcTEYlerU8qO+d+DXQmdj5hrHNuph8aCZwEFAGt417SGtiSpjxFRCRiSQuCmV1pZn/0H24HKoF3zOw0/7lewExgMtDbzHLNrAOQ65wriCJpERFJv9qcVH4HeMnMJgLNgJuBVcBTZlYKrAWuc84VmVkeMJVYoemfriTV/bWISPSSFgTn3DbgFwlCZySYdjAwuN5ZBdG5BBGRyOjGNBERAVQQRETEp4IgIiKACoKIiPhUEEREBFBBEBERX3YVBN2PICISmawoCOrLSEQkellREEREJHoqCCIiAqggiIiITwVBRESALCkI6u1URCR6WVEQqulqIxGRyGRXQRARkcioIIiICKCCICIivqQjpplZE+AFwIAK4GpirfkvE+tMYh7Q3zlXaWaDgL5AOXCzc256RHmLiEia1eYI4ccAzrkzgbuBIf7fQOdcd2LF4UIz6wL0ALoClwFPR5KxiIhEImlBcM69C1znP+wIrANOBib4z40BzgW6AWOdc55zbiXQ1MzapTVbXX4qIhKZWp1DcM6Vm9krwJPACCDHOVe1eS4G2gJtgMK4l1U9X2/q3E5EJHq1PqnsnPs10JnY+YSWcaHWwBagyP9/9+dFRCQLJC0IZnalmf3Rf7gdqAQ+N7Oe/nN9gDxgMtDbzHLNrAOQ65wriCBnERGJQNKrjIB3gJfMbCLQDLgZWAC8YGbN/f9HOOcqzCwPmEqs0PSPKGcREYlA0oLgnNsG/CJBqEeCaQcDg+udlYiIZJxuTBMREUAFQUREfFlRENT9tYhI9LKiIFTT/QgiIpHJroIgIiKRUUEQERFABUFERHzZVRB0cllEJDJZURDUuZ2ISPSyoiCIiEj0VBBERARQQRAREZ8KgoiIACoIIiLiU0EQERFABUFERHyhA+SYWTPgRaAT0AK4D8gH/gMs9id71jn3hpkNAvoC5cDNzrnpUSUtIiLpl2zEtH7ARufclWZ2ADALuAcY4px7tGoiM+tCbAS1rkB74G3g1HQlqe6vRUSil6wgvAWMiHtcDpwMmJldSOwo4WagGzDWOecBK82sqZm1c85tSGu2umNZRCQyoecQnHNbnXPFZtaaWGEYCEwHbnPOnQUsAwYBbYDCuJcWA22jSVlERKKQ9KSymbUHxgOvOeeGAyOdczP98EjgJKAIaB33stbAljTnqs7tREQiFFoQzOwgYCwwwDn3ov/0h2Z2mv9/L2AmMBnobWa5ZtYByHXOFaQrSXVuJyISvWTnEO4E9gPuMrO7/OduAf5mZqXAWuA651yRmeUBU4kVmf5RJSwiItEILQjOuZuAmxKEzkgw7WBgcFqyEhGRjNONaSIiAqggiIiITwVBREQAFQQREfGpIIiICKCCICIiPhUEEREBsqQgqLdTEZHoZUVBEBGR6GVFQVBfRiIi0cuKgiAiItFTQRAREUAFQUREfCoIIiICqCCIiIhPBUFERAAVBBER8YWOmGZmzYAXgU5AC+A+YD7wMrEh7+cB/Z1zlWY2COgLlAM3O+emR5e2iIikW7IjhH7ARudcd6AP8BQwBBjoP5cDXGhmXYAeQFfgMuDp6FIWEZEoJCsIbwF3xT0uB04GJviPxwDnAt2Asc45zzm3EmhqZu3SnayIiEQntCA457Y654rNrDUwAhgI5DjnqrqbKwbaAm2AwriXVj0vIiJZIulJZTNrD4wHXnPODQcq48KtgS1Akf//7s+nlYe6PRURiUpoQTCzg4CxwADn3Iv+07PMrKf/fx8gD5gM9DazXDPrAOQ65woiyllERCIQepURcCewH3CXmVWdS7gJeMLMmgMLgBHOuQozywOmEisy/aNINgd1eyoiEpXQguCcu4lYAdhdjwTTDgYGpyUrERHJON2YJiIigAqCiIj4VBBERARQQRAREZ8KgoiIACoIIiLiU0EQERFABUFERHwqCCIiAmRZQVDndiIi0cmKgqA+jEREopcVBUFERKKXFQVBTUUiItHLioJQRU1HIiLRyaqCICIi0VFBEBERIPmIaQCYWVfgIedcTzPrAvwHWOyHn3XOvWFmg4C+QDlws3NueiQZi2S5LdtLadm8CS2aNmnoVER2kbQgmNntwJXANv+pLsAQ59yjcdN0ITaKWlegPfA2cGrasxXZA5x4z0ec2mk/3rr+jIZORWQXtWkyWgr8LO7xyUBfM5toZsPMrDXQDRjrnPOccyuBpmbWLoJ8RfYIM1ZsbugURGpIWhCcc28DZXFPTQduc86dBSwDBgFtgMK4aYqBtmnMU/YQk5cUULi9LPmEIpJxqZxUHumcm1n1P3ASUAS0jpumNbClnrnJHqa4pIwrhk7jN69+3tCpiEgCqRSED83sNP//XsBMYDLQ28xyzawDkOucK0hXklV0g1p2K6+ILb9F64sbOBMRSaRWVxnt5nfAU2ZWCqwFrnPOFZlZHjCVWJHpn8YcdUPaHsarR11/cMxCmubm8H+9LX0JiQhQy4LgnFsBnO7//wVQ4/II59xgYHD6UpM9TU4a6vpzE5YCqCCIREA3pklaffjftZSWV4ZO49XnEEFEIqOC8C0yaXEBT32yOPmEKcpbvIHfvjaTxz5elDCupj/JJm/PzGfrzvLA+KXPT2XM3DUZzCh6WVEQoj6ZfP7jeTzz6ZJIP6Mx6DdsGo+MTbyxTodN20oBWL15R+h0YUuz39Bp/H3i0jRmJdnmjRkrWVtY0qA5fLFyM7e+NYeBI+cGTjNt+SZ+9/oXGcwqellREKpEtYc5f00RD3/gInnvdDr8zve59pUZDZ1G6mqx+CYtKeD+9xdGn0sCnuepOauetu0s54WJy6isTG0+bty6kwFvz+Wql4J7vrl9xBxGzsoPjA96bx4PvL8gML5leymrNm0PjFdWemzaGtu52bB1Z414aXklkxaHX0T51cZtofHGKqsKQmNVUlZBcUn0N1tVVHp8vGB95J+TipKyCjYU1/zxVFlbWMKT4/zmqgTbih2lFTwwJvhHXFpeyQ3Dw/fG7hs1nwEjvmTgu4n36s5/PI8/vhO8x9fr0QkcffcHgfF7R83nsD+ODox/MG8txw/+kJKyioTx/M3bufjZKYGvT4dLnpvCkLHBOzdvfr6Kf05fGRj/y+j5nPHAuMD4hEUb+PmzU6gI2ODf//4C/vL+Aj5asC5hvKSsglvemM364sRHAFXvu9E/2kzkzc/z+cMbcwLjr0z9iucnLguMd39oPN0fHs+dI+eyvbRmk9Bd783j2pB7ZR4cs5B+w6YFxictLqDHXz/lnS+Ci1ZjpYKQBn0ez+O4wWMD4wvXFvHsp9E2g3wwby1FIUVpaF7wDwRg/tdFrCgI3quZvKSA92avDoz/5tXPuW908Ab9xn9+wdBJywPjf5+4jOcnBOf4xcrNjPoyvL126KTlvPH5Kv7xWeIN3vw1RaEbw2UF2ygpCz4hPmzScsJ2fO9/fwFFJeWsK0q8sXvm06XM/Cq4y4r1RSV0umM0ExdtSBivrPT40d8m8n5Iu/WMFZt54pPg5s/bR3wZWhRfyFvO1yHNNb//5yw+/2pz4A5QUUlsAxtUFEd9uYZ3Zq3mwTENcxQIUOyfFxg+bSWvTv2qRnx43DqSqFVi6Yatoe/v1sXus5m7ujB0usZIBSENlodsSAEueGISD30Q/AOYsqSAI//0fmCXDgVbd3LBk3mhn3/9P2Zy65vBe01hG2uA85/Io+cjnwbGrxg6jZv+NTswnpfkEHrbzm82EIm2qaUViTcg3yazVsVu7n/ts5obKYCd5ZUsXFvMLW8GL4eskaRFKVMtd2oh3JUKQgaUJ2lPfWr8EsoqPOZ9nXiP4u2Z+cxbXRT4+qrD3vwkJ3PTYfSXaxqknV3XJ2WHquUUtIokXY5ZsKDTcT9NY7XHFIQt20vZFnKJWDZIdTubye1z/+Ff8MnCup/HiE+xoU/cTlmS9l5VaiXZ166KfzR/HRMCmo0au9puLLNlxzyVjX/V+v3FV5uTNi81NntMQTjxno/o/vD4SN7733O+Zux/10by3pC+PY5M7bhsjqC30mRXkOWkcbfs8qHT2FmeehPVvNWFoUXtiqHTmLFiU8rvD/DrF+s3vtT05fX7/CC1LeZBl4pnw951sq9Y268wJ7+QXo9OqHc+mZRVBSHZ/QibQq5MqI23Pl+V8Pnf/3MW1702M2EsnV7IW8bidY2/47dU9vAb+qhgd/VJ54InJ/GPBO38VRu7/M07Qs/n1FdJWWXSLsR/8fzUyD4fggt41bN/eGMOj38cfBNk8vUh+QLKxJV9iaRz56SxyYqCkKk7XG8b8WVGPifIhEUbuOjpyQ2aw5PjFke+8U7l3TP5G9xRmvzowaVUuOu/Eaxywj3BV7XVRnlFePci05ZtrPd6kOiO9WTLseq3XrC1lNVbws+JhV3ZB9TqKE09KO8qKwpCGM/zQi8lrDLzq02Ntl02vuDtCLhcr1bvk4aN5qMfLQq9nwCiaf9tTDtdR9/9AcuyrO23ro740xjWFAZvcC/9+2e8GXDEDPDxgnUJu3Wo7d5zbdahMx/8pFbvFeSS56aG3oCWqka0qqZd1heE0XPXhF5XXeXiZ6fWu122oST7jVXtyG3dWU7hjoYfjSyKjXu63zLZzu/i9Xt2QQD4amP4xnJFSPzWt+YwIIUj6qqdn8lLNjI3P/rr9ItLsvtCk0zLuoKwatP2XdpPa7PAt2yv37mFqMVvQOuz9/3Vxu2c8Of6NSXUKocUkmxkpxCSSiXfZEWrtlcZZUoq+cQ/9dWmmvffJJsHVet6wdad/PipSYHxdKlvk1CiI57GdDSbbllXELo/PJ7zHqv9mfuikjJOvOejCDNqeOluB416w5To/Wu7Icmc8JmgnltlT5R1BQFgfVwbd7KfZVEjaELZ06RSgLLt5F22HdE0GntYnQy/liqxbF53alUQzKyrmX3q/3+EmU0yszwze9bMcv3nB5nZdDObEjfmclq0atEEgDmrGrZvkGQ3vv3iualsDrn09V/TV4b2rw6NY2Wq92F2oveMe8vU3j+9W5pkOTTEYkj3Zya7Sqi+y7kxrKvJZEOOjUnSgmBmtwNDgb38p4YAA51z3Yn9Si80sy5AD6ArcBnwdDqTPKLdPgBJL0OL2g8GfRi6wZ++YhMjZwV3AHfHO3O5+715NZ6v73XNja3teU+Q0jmEJMvx27Cc6nuDYaYPMJLehJYgoW/7OYSlwM/iHp8MVDXijwHOBboBY51znnNuJdDUzNqlNdMAmV44Yd3yQvK9vFRunst0e3UUG5Jdu65IMEGyDUWGl3Oyvec9YqOQynJOYQOazRrfua1oJS0Izrm3gfiG+BznXNVqUQy0BdoA8e05Vc/LbhKtP/VdpzK9I7gnHCA0tit+ovjM+r5doqJYnKTJc0mSy3Ub2/YzlQ36wrXBHU1C9p0vi5fKSeX4WxxbA1uAIv//3Z+P1KduPQPeTn4PQiYla7fNhtveo1id4+dLw59BSC6KjWmmNxSR33Ge4O0TDThTF9nw+1izJXx4zxQHi2sUUikIs8ysp/9/HyAPmAz0NrNcM+sA5DrnIu9SsiG6mqjv+pro5VvqeSVUY+snKJnS8prdJjS2zUDSwp7k9WXl6V8mxTvrtp6kr6OM2lu0LvwIIdnQk41NogKVrDv7bNY0hdfcCrxgZs2BBcAI51yFmeUBU4kVmf5pzHGPkqigzFkV+cFUnSS9OiWVG9NSzKVKuvcco76bujLBTEraTJVkLiXr0K6un9cQ5q8Jb27JNglv3muE8722alUQnHMrgNP9/xcRu6Jo92kGA4PTl9o3gn68DbFXWf/PbGz7wtGrrPRCB12vqPQCh1yEWIFaG9LvDtS9qaKuP9rdO4NLVKCWJRk5b0WGB17P5rbsxiKVX2s2z/esvDENYn3SJ7umvyGkchlbMsleEzYOMMB4Fz6gTbLxlq95ecYuj3df4XfvhGz3jeUvX/gstF+cge/OCx0Ufdik5Vz/jy8C41+s3Mwxd38YGC8qKaPTHaMD40CN+O7L8ciBY0Jff8Pw4Pwg1ovsjBXB4ym/O2s1NwyfFRif/3UR5z02MTBeuL3md9z9OySbB2HDtAL8JEFXE/GGjHWh8Xe+yA8dZ3jOqi10uTe4V4GNW3eGfgfP87hz5Nzdntt1mtG7jcu9e/yZT4PHowZCOwSEWNc6w/KCxw5v7FJpMmoULngy8cr53uzV7NeqOWd1bseO0gqGTwvuCbW8opK/hfTZPje/MLTaPz9hKZNCRt/6ZOE6nhi36woWv6mcsqSAy4dOC3z9+qISTrt/XGC8stLjxHvGVg9snsig9+bxSoKBxKssWV+cdLzlZCOkJbs/ZFqSwVremBHeW+3EJO3Os1eGN7mtDxj0Pszuyz1ZoR+1+4Zmt/ijH9XsCjreq1NXhMY/SDJA05INde+Oe/fvFDZMK8CXSTqje+KT8I3p0+PD4+/N/jo0nmzQ+kqP0N87xEb8C/PwB+FFrcfDn4bGf/3S9KSXpjdmWVEQ1hWFd8ccr2og+Kcv7xK48L/M38KYeWsZNml5whOcnudRuKMsoPOtbzbpD4xZWPO1cZuCm/45u8Zletvimjb+MS14Qw3wqQvvrnttUUnCYuB5XnWeiYpBZaVHbm4sfu6Qmnud8RuKpz6pWTDj48kG9KnNsKbJztEl6pywotKjif8dNm6ruX7sKK2gZfPYHe5rCmsWhE3bStm7RdPAHOPXuUTjI8S/JtE6FB+vqMVJyC+SFLVk3XEv3VCzOcrzYHnBNr63b8uEveBWeB6L1xXTrnUL9mrWpEa8cHsZRSVlNMnJoVXzmvH1xSVs2hYburb9/q1C8wtSXlFJ0ybBDRXri0to27IZLZo2SdhM9/mKTZzSaf/A1z81fjHPX3lKYPyvHzr6n31EYHzcbjtDpQnGkajKsaLSY2uC3+N4t56z7cDAz2hMsqIgpNJbadiewE+eCh6EZt7qQu56bx6zAn6gU5YW8KeRc+l4QOIfwP3vL2TiogIuOP6QhNdsT16ykdf80bben5t4r++j+ev4ZOH6wHEe1heXMHFRAU+MS3x0s3TDVtq2bJ5wVC+At7/I54fHHMwtb85OGH96/BIe+NlxvDxlBY+MrblnO/DdefQ7vSOzVm7mp89MqREfOWs1j116Iqu37OBPIxNfFrxleykFW3fynzlrEsYnLS7g6ENaMyd/S8I90yEfOW4850jmri7k6fFLa8SvHDaNEb87g5KyCq4cVrPb8+4Pj2fFg30B6PZQzX73HxyzkOt7HA7AVS/VfP1bM/P56yUnAHDvqPk14ttLK1hTuIPS8krGLUh8hJW3eAMrCrYFbtA+nr+Oz5Zt5I4+R9U4AgG4+715vDr1K4Zf25XbE1xx12/YNGZ+tZlLT2nPGwnGNqjqDv57+7bkwDYtasT/NWMV/5oRe90t53WuES/YWsqpf/mYikqPJ395UsLvcNtbc3h39mpGXH9GwqL1k6cmM39NEX86/2henFyzqeW0v8SOkI/7Xlv237t5jfjPn5vKAXs3Z+O2Uu6+4Jga8Q//u44HxyzkuQlL6X/24QlzfHfWam5+YzY/POaghHG3tpjef5vIMYe0SRj/clUh1776ecIYwKcL17NpaykXn3xo4DSNRU6mLlk0s07A8nHjxnHooXWbMZ8sXMc1LwfP8LoY+qtTQhfet8G+rZqxJckVKwN+dBQPfVDzCKjKU5efFNrm/dilJ/CHN4KHkfxtj8N4fkLweYO2LZslHduhdYumoTdK9T3uEEbPTVxwAO658Afc/d5/A+MPX3w8r09fGXgV2C9P61CrwZmS6XPswYyZF92Y3dI4fHxLD444cJ86vy4/P59evXoBfN+/wCcyWVEQxi9cz9W7ndgUEckmo3/fjR98t+4dOGSyIGTHVUbfvis1RUQyLjsKgoiIRC4rCoIOEEQk241Pcvl2Y5AdBSELOrwSEQmT6Iq9xiYrCoKIiEQvKwqCjg9ERKKXHQVBFUFEJHLZURB0jCAiErnsKAiqByIikcuOgtDQCYiIfAuk3Lmdmc0CqnodWw48DzwOlANjnXN/rn96PlUEEZHIpVQQzGwvAOdcz7jnZgMXA8uA0WbWxTkX3vm4iIg0GqkeIZwAtDKzsf57DAZaOOeWApjZh0AvQAVBRCRLpFoQtgOPAEOBI4ExQHwfwcXAYfVL7Ru6ykhEJHqpFoRFwBLnnAcsMrNCIH6Uj9bsWiBERKSRS/Uqo2uARwHM7LtAK2CbmR1uZjlAbyB8xG4REWlUUj1CGAa8bGaTiI0nfg1QCbwONCF2lVHw6PEiItLopFQQnHOlwOUJQqfXL53EWiYY4FtERNIrK25MO+57dR92TkRE6iYrCoKuMRIRiV52FARVBBGRyGVJQVBFEBGJWlYUBBERiZ4KgoiIACoIIiLiU0EQERFABUFERHwqCCIiAqggiIiITwVBRESALCoI6s9IRCRaWVMQWqnHUxGRSGVNQVDvFSIi0cqagvDIJSc0dAoiInu0rCkIh+7XiuUPnB8Y/8Uph9LrqAMzmFHdfbftXqHxdq1bhMbbtmwWGm+S2/CHUYd9Z+/Q+PGHhp8LOqxd+Osbgx98t01o/LxjDgqNn9B+39D4IUnWk3Ro2Sy8CfbnJx8aGr/xnCNC432PPyQ03mH/VqHx1i1SHczxG8ccEr6cLu/aITT+0MXHhcb/p9v3Q+N9jj04NN4YpbUgmFmumT1nZlPN7FMzC19r6ignJ4e5g3/I/T89jktPaV/9/JWnd+TO849m2FWnBi7kszq3Y8WDfXnlmtMC33/yHedweMgGadSN3ULzm3jb2aE/pP/c2I2rzugUGH/kkhO48/yjAuOXndaeB38WvJLaQa158apTQnNM9h0++sNZofHPB57LPiE/1vduOJPTvr9/YPzeC4/lhrODV4tLT2nP73oeHhg/29rxaMjR4j4tmjLu1h6BcYDZd58XGl94749C4+/87xmh8bsvOCZ0Y3RF1w7c1OvIwPjphx3Ax7cEL4eD2rRg/j29Q3MI23kCWHDvj2i9V/ByTHZEfm33w5h+Z6/A+EUnfo/Ff+kTGO919IGseLBvYLz9/q1C4wArHuzLgSE7Ue/f1J3uR34nMH7/T4/jqINbB8YvObk900K+4wXHH8LcwT8MjJ/UYV+W3R++HBqbHM/z0vZmZvYz4CfOuavM7HTgj865C/1YJ2D5uHHjOPTQ8L2P2pi9agsXPT2ZLh325Z3/PbNGvLyikiP+NAaAl68+lS4d96PNXrE97KKSMlo2a8KRfvyxS0/ADmrDMf6e36ZtpeTmwIn3fATAM1d0oWXzJpxtsSOQzdtKceuKuezvnwEw/NqufF1YUl0MNm0rZU7+Fq5+aQYA91z4A8oqvOo9itLySiYvLaiOt2rehN+edTi/73UEOTk5eJ7H6LlruGH4rOrv88vT2vPH84+u/g5TlhawaVtp9TRXndGJq8/sRMcDYgVtyfpi/j1nDU+MWwzA7885gm5HtqveWC9ZX8zIWat5evxSAO696Fi+s3dz+hwX27NbW1jCio3bqr/jc/1OpmDrTvqd3rF6Hm7ZVsZZfx1f/f7Nm+ZywzlHVn/H/M3bOefRCQB8Z58WXHLKodze26q7M19XVELX+8dVf8dru32f/+tt7NWsCVt3lrOuqIT5Xxdx4z9j3/Hmc4+k3+kd+c4+LRg+bSUnddiXB8YsZOKiDQA88LPj6Pr9/Tms3T5s21lOeYXHK1NXMOSjRdXf4cA2LejSYT+KSsqoqPCYk7+Fq/zlMPw3XSkpq+Ccow6qXofKKjyOvvsDAEZc//9YtXk7Pz1p15eviTwAAAnaSURBVPW30x2jq5dRh/335voeh+F5kL95Bx0OaEWfx/NYsKaIti2bcdUZnbjxnCNo2iSXReuKObzdPhx+5/vV73Vbb6Pf6R13ORo85b6PKdi6E4D7LjqWs45sR4cDWlG4o4wWTXN5dKzjhbzlADx9eRc67N+K4+KOxGat3MxPn5kCwGv/cxqVHvTo3A6AykqPwh1lnHTvR9XfcU1hCT8+4bvsKK1gy45SDmnbsvo7/rbHYRz2nb259NRdd7yq4ie035cLT/guV5/ZaZdu66viB+zdnN/1PJwrunasHhq3pKyCo+76oHrahy4+jp52IAe1+eZI6VcvTq9ezn+/8mQOa7c3Rxz4zcZ8xMx8/u+tOUCsYDfLzd1lHizbsLV6XRx3aw8KinfS9bADEn6Hey86lkP3a1n9e9893v/swzmx/X41jgSr4gDP9evCecccXH3UXlHpkZuTWlf++fn59OrVC+D7zrkVdX6DuvA8L21/nTt3HtK5c+fL4h6vjvu/U+fOnb1Vq1Z56bKzrMKrrKwMjN8/er43ZUlBYPyxj5w3as7XgfGOA0Z5HQeMCoxf9+oMzwa+Hxi/5qXpXscBo7ySsvKEcbe2yOs4YJS3cevOhPGFa2LxMXMT51hZWel1HDDK+/O//5vSd6h6/SXPTgl8/cXPTA6dB8fe/YHXccAob9vOsoTxCW6913HAKC9/8/aE8fdmr/Y6DhjlPfD+goTxsvIKr+OAUd7Fz0xOGN9aUuZ1HDDKO/PBcQnjVd/x7L+OD/wOyZZzVbysvCI0vmBNYcL4zrIK78KnJnk7ShOvB58tLQhdzss2bPU6Dhjl3f7WnMD37zhglHfekE8Dv0O3h8aFfsd+Qz8LjdvA972OA0Z5a7bsSBh/Y/pKr+OAUYG/t7n5W7yOA0Z5Q/OWJYyvK9zhdRwwyvvDv2YljBftKA1dThUVseV8+B9HB36HZMv5kmeneB0HjPIKd5QmjL85I/YdF64pShgf8fkqr+OAUd5N//wi8DNSsWrVKq9z585e586dO3lp3F4n+kv3EcJQ4G3n3Bj/8UrgMOdcebqPEKR2Kis9clLcM4HYDkOlF+35iZ3lFbRomr2XFXueR1mFR/OmjfeUXEVl7AfftEniHCsqPXaWV9Cqef3b7oMkWxfru67W5vMrQ+ZBY5XJI4R0L/0iIL5RLtc5V57mz5A6yK3nhjwnJ4cmEZ+rzuZiALF51Lxpw5/QDxMr6ME5NsnNibQYQPJ1sb7ram0+P1cjtIdKd6mcDJwP4J9DmJvm9xcRkYike5dgJHCemU0htjtydZrfX0REIpLWguCcqwSuT+d7iohIZmTX2RUREYmMCoKIiAAqCCIi4ov2OrNdNQFYu3ZtBj9SRCS7xW0zI78+O5MF4RCAK664IoMfKSKyxzgEWBrlB2SyIMwAugNrgIoMfq6ISDZrQqwYzIj6g9LadYWIiGQvnVQWEREgs01GKTGzXOAZ4ARgJ3Ctc25JBj+/GfAi0AloAdwH5AP/ARb7kz3rnHvDzAYBfYFy4Gbn3HR/TIiXAQ+YB/R3zlXWZdp65D4LKPQfLgeeBx73P3Osc+7PQfPX73ok5WlTzPcq4Cr/4V7AicDlwF+BVf7zg4C8RpRzV+Ah51zPdCzr+k6bQs4nAk8Sa8bdCfzKObfOzJ4AzgSK/ZddCDQDhgMtga+Bq51z283sN8Bv/Tzuc86NMrPv1HbaFOd1FzL4G0x1XifI+19A1cg5nYDPnHOXmdm/gQOAMmCHc65PQ+SdDUcIFwF7Oef+H3AH8GiGP78fsNE51x3oAzwFdAGGOOd6+n9v+CtoD6ArcBnwtP/6IcBA//U5wIV1mTbVpM1sL4C4HK8GniO2ge0GdPXzCJq/9Z22zpxzL1flC8wEfk9sXt8e9z0mNJaczex2YCix4gX1XNZpmrauOT8O3OjP83eAAf7zXYDecfO9ELgbGO7nMQv4rZkdTGw5nQn0Bh4wsxZ1nDaVvDP2G0x1XifK2zl3mT+vfwpsAf7gT3oE0M3/LlUjC2U872woCN2ADwCcc58B4UOCpd9bwF1xj8uBk4G+ZjbRzIaZWWs/z7HOOc85txJoambt/Gkn+K8dA5xbx2lTdQLQyszGmtknZnYW0MI5t9Q55wEfAr1IMH/NrE0apk2ZmZ0C/MA593di8+QaM8szs0fNrGkjynkp8LO4x/Vd1umYtq45X+acm+3/3xQo8Y+qjgT+bmaTzewaP149L+PyOA2Y7Jzb6ReNJcDxdZy2NhLN60z9BlOd14nyrvJn4Enn3BozOwjYF/iPmU0yswvivmNG886GgtCGb5o9ACr8jUJGOOe2OueK/RVuBDAQmA7c5pw7C1hGrBlj9zyLgbZAjr/RiX+uLtOmajvwCLE9seuBl/zndv/MGvPXf66ontPWx53EfjAAHwE3AmcB+xD7Lo0iZ+fc28QO8avUd1mnY9o65eycWwNgZmcANwCPAXsTa0bqB/wI+F8zO363z0yWR12mTSrBvM7kbzCdeWNmBxLbAXnZf6o5sSPXi4gVj8f8aTKedzYUhAYfY8HM2gPjgdecc8OBkc65mX54JHBSgjxbEzskrEzwXF2mTdUi4B/+3sEiYitG/GDHQbnkhuRXl2lTYmb7Akc558b7T73onFvmr+zvkXheN2jOceq7rNMxbZ2Z2aXEmtD6Ouc2ENtxeNw5t905Vwx8QuyIM/4zk+VRl2lTkcnfYLrXl58Ta06ruvx+LfCcc67cObeeWBObNUTe2VAQGnSMBf9wbiwwwDn3ov/0h2Z2mv9/L2Lt3ZOB3maWa2YdiBWuAmCWmfX0p+1D7IRoXaZN1TX47eVm9l2gFbDNzA43sxxiRw5Vuewyf51zRUBpPadN1VnAx/5n5ABfmlnVEHvx87ox5Vylvss6HdPWiZn1I3Zk0NM5t8x/ujMwycyaWOyiim7AF8TNy7g8pgPdzWwvM2sLHE3spGZdpk1FJn+DaZnXcc4l1qwT//hNADPbBzgWWNAQeTf6q4xo+DEW7gT2A+4ys6pzCbcAfzOzUmLV/TrnXJGZ5QFTiRXa/v60twIvmFlzYgt5hHOuorbT1iPvYcDLZjaJ2JUH1xDbi3id2I0uY51z08xsBonn7/X1mbYeeRuxJgCcc56ZXQu8Y2Y7gPnAC8SafRpTzlXqtazTNG2tmVkT4AlgJbF5DDDBOTfIzF4HPiPW3PGqc+6/ZnYf8IrFrhQqAC53zm2z2BVJeX4ef3LOldRl2rrm7fsd8FQmfoPpmNe7qV7HAZxzY8yst5l9Ruw3eqdzrsDMMp63bkwTEREgO5qMREQkA1QQREQEUEEQERGfCoKIiAAqCCIi4lNBEBERQAVBRER8KggiIgLA/weDeXB0n8NnfQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "  very glants to thee, holy.\n",
      "\n",
      "DUSEW:\n",
      "I find cold to kings?\n",
      "\n",
      "LADY YRY:\n",
      "O, a prinag; good days not,\n",
      "as leave whereffit his possing you.\n",
      "Shall inform laakts of your mistress.\n",
      "\n",
      "KING RICHARD II:\n",
      "So land; do \n",
      "----\n",
      "iter 1668545, loss 34.073613\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        with DelayedKeyboardInterrupt():\n",
    "            # Reset\n",
    "            if pointer + T_steps >= len(data) or iteration == 0:\n",
    "                g_h_prev = np.zeros((H_size, 1))\n",
    "                g_C_prev = np.zeros((H_size, 1))\n",
    "                pointer = 0\n",
    "\n",
    "\n",
    "            inputs = ([char_to_idx[ch] \n",
    "                       for ch in data[pointer: pointer + T_steps]])\n",
    "            targets = ([char_to_idx[ch] \n",
    "                        for ch in data[pointer + 1: pointer + T_steps + 1]])\n",
    "\n",
    "            loss, g_h_prev, g_C_prev = \\\n",
    "                forward_backward(inputs, targets, g_h_prev, g_C_prev)\n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "            # Print every hundred steps\n",
    "            if iteration % 100 == 0:\n",
    "                update_status(inputs, g_h_prev, g_C_prev)\n",
    "\n",
    "            update_paramters()\n",
    "\n",
    "            plot_iter = np.append(plot_iter, [iteration])\n",
    "            plot_loss = np.append(plot_loss, [loss])\n",
    "\n",
    "            pointer += T_steps\n",
    "            iteration += 1\n",
    "    except KeyboardInterrupt:\n",
    "        update_status(inputs, g_h_prev, g_C_prev)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated text does resemble shakespeare's texts in some ways. It has also learned the words that it can make up with the available charachters. So we can say the LSTM has captured the features that make up a shakespearean tale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
